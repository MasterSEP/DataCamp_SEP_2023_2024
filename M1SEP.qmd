---
title: "DATA Camp M1"
editor: visual
---

Ici, seront list√©s les pr√©-requis n√©cessaires pour le master SEP.

# Math√©matique

Nous allons commencer par les bases des probabilit√©s, de l'alg√®bre lin√©aire et des statistiques.

## Alg√®bre Lin√©aire

Rappelons les notions essentielles √† conna√Ætre sur les matrices.

### Trace d'une matrice

::: custom-box
La **trace** d'une matrice carr√©e $A$ est la somme de ses √©l√©ments diagonaux : $$\text{Tr}(A) = \sum_{i=1}^{n} a_{ii}$$
:::

Propri√©t√©s : - La trace est lin√©aire : $$\text{Tr}(A + B) = \text{Tr}(A) + \text{Tr}(B)$$ - Invariance par similitude : $$\text{Tr}(AB) = \text{Tr}(BA)$$

::: callout-note
La trace d'une matrice ne change pas lorsqu'on change de base.
:::

### D√©terminant d'une matrice

::: custom-box
Le d√©terminant d'une matrice $A \in \mathcal{M}_{n\times n}$ peut √™tre calcul√© en utilisant le d√©veloppement par rapport √† une ligne ou une colonne : $$\text{det}(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \text{det}(A_{ij})$$ o√π ($A_{ij}$) est la matrice obtenue en supprimant la ($i$)-√®me ligne et la ($j$)-√®me colonne de ($A$).
:::

::: callout-tip
Dans certains cas, il peut √™tre long de calculer le d√©terminant d'une matrice. Pour rem√©dier √† cela, il est utile de conna√Ætre par coeur le calcul d'un d√©terminant d'une matrice de taille $2 \times 2$ et d'une matrice diagonale.
:::

### D√©terminant d'une matrice $(2 \times 2)$

Pour une matrice $A \in \mathcal{M}_{2 \times 2}$ de la forme : $$A = \left(\begin{array}{cc} a & b \\ c & d \end{array} \right)$$ le d√©terminant se calcule en utilisant la formule suivante : $$\text{det}(A) = ad - bc $$

### D√©terminant d'une matrice diagonale

Pour une matrice diagonale de la forme : $$D = 
\left(\begin{array}{cccc}
d_1 & 0 & \cdots & 0 \\
0 & d_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & d_n
\end{array}  \right)
$$ le d√©terminant est le produit des √©l√©ments diagonaux : $$\text{det}(D) = d_1 \times d_2 \times \cdots \times d_n$$

Les d√©terminants sont particuli√®rement utiles en alg√®bre lin√©aire pour d√©terminer si une matrice est inversible (une matrice est inversible si et seulement si son d√©terminant est non nul) et pour r√©soudre des syst√®mes d'√©quations lin√©aires.

### Espace Euclidien

::: custom-box
Un **espace euclidien** est un espace vectoriel r√©el de dimension finie muni d'une forme bilin√©aire sym√©trique d√©finie positive, appel√©e **produit scalaire** : $$\langle x, y \rangle = x_1 y_1 + x_2 y_2 + \cdots + x_n y_n$$
:::

Quelques propri√©t√©s importantes du produit scalaire : - $\langle v, v \rangle = \|v\|_2^2 = \sum_{i=1}^{n} v_i^2$ - $\langle v, w \rangle = v^t w = \text{trace}(vw^t)$ - $\langle v, w \rangle = \langle w, v \rangle$ - $\langle v, w \rangle = 0 \iff v \perp w$

Quelques in√©galit√©s √† conna√Ætre :

-   In√©galit√© de Cauchy-Schwarz : $$|\langle x, y \rangle| \leq \|x\| \|y\|$$
-   In√©galit√© triangulaire : $$\|x + y\| \leq \|x\| + \|y\|$$

### Orthogonalit√©

Soient $v$ et $w$, deux vecteurs appartenant √† un espace vectoriel $E$ de dimension $n$, muni du produit scalaire $\langle \cdot, \cdot \rangle$ associ√© √† la norme euclidienne $\|\cdot\|_2$.On note $v = (v_1, \ldots, v_n)^t$ et $w = (w_1, \ldots, w_n)^t$ o√π \$ v_i\$ et $w_j$ sont des scalaires (i.e. des r√©els).

#### Normalisation d'un vecteur

La normalisation d'un vecteur $v$ est donn√©e par : $$
x = \frac{v}{\|v\|_2}
$$ Ainsi, $\|x\|_2 = 1$. Un espace vectoriel engendr√© par $v_1, \ldots, v_p$ est identique √† celui engendr√© par leurs versions normalis√©es.

Deux vecteurs $x$ et $y$ sont **orthogonaux** si leur produit scalaire est nul : $$\langle x, y \rangle = 0$$ Une **base orthonormale** est une base o√π tous les vecteurs sont orthogonaux et de norme 1.

#### Matrice de Projection Orthogonale

::: custom-box
Soit $E$ un espace vectoriel muni d'un produit scalaire $\langle \cdot, \cdot \rangle$ et $W$ un sous-espace vectoriel de $E$. La projection orthogonale d'un √©l√©ment $B$ de $E$ sur $W$ est d√©finie par : $$\Pi_W B = \underset{a \in W}{argmin} \|B - a\|_2$$ La matrice de projection orthogonale de $E$ sur $W$ est not√©e $\Pi_W$.
:::

##### Propri√©t√©s

-   $\Pi_W^t = \Pi_W$
-   $\Pi_W^2 = \Pi_W$
-   $\text{trace}(\Pi_W) = \dim(\Pi_W)$
-   $\Pi_W^\perp = I_E - \Pi_W$

Pour tout $B$, √©l√©ment de $E$ : $$
\Pi_{W^\perp} B = B - \Pi_{W} B
$$

#### Construction de matrice de projection Orthogonale

::: custom-box
Soient $v_1, \ldots, v_p$, $p$ vecteurs de $\mathbb{R}^n$ avec $p \leq n$. Si $(v_1, \ldots, v_p)$ est une base orthogonale de $\mathbb{R}^p$ et $W = \text{vect}(v_1, \ldots, v_p)$, alors la matrice de projection $W$ de $\mathbb{R}^n$ sur $W$ est : $$
W = V (V^t V)^{-1} V^t
$$ o√π $V$ est la matrice dont les colonnes sont les vecteurs $v_1, \ldots, v_p$.
:::

Si $(v_1, \ldots, v_p)$ est une base orthonormale de $W$, alors : $$
W = V V^t
$$

#### Proc√©d√© d'orthogonalisation de Gram-Schmidt

Le but du proc√©d√© de Gram-Schmidt est de prendre un ensemble de vecteurs lin√©airement ind√©pendants $( {v_1, v_2, \ldots, v_n})$ et de produire un ensemble de vecteurs orthogonaux $( {u_1, u_2, \ldots, u_n} )$ qui engendrent le m√™me sous-espace.

::: callout-tip
Avant de recourir au proc√©d√© de Gram-Schmidt, il faut s'assurer que les vecteurs ne soient pas d√©j√† orthogonaux, cela serait une perte de temps de les recalculer. Ce calcul est long, il faut bien prendre son temps pour le faire et ne pas se pr√©cipiter.
:::

**√âtapes du Proc√©d√©**

Le premier vecteur orthogonal ( u_1 ) est le premier vecteur de l'ensemble original : $$u_1 = v_1$$

Pour chaque vecteur $( v_k )$ (o√π $( k \geq 2 )$), on soustrait les projections orthogonales de $( v_k )$ sur les vecteurs orthogonaux pr√©c√©demment calcul√©s $( u_1, u_2, \ldots, u_{k-1})$ : $$ u_k = v_k - \sum_{j=1}^{k-1} \frac{\langle v_k, u_j \rangle}{\langle u_j, u_j \rangle} u_j $$

Si l'on souhaite obtenir une base orthonormale, chaque vecteur $( u_k )$ est normalis√© pour obtenir $( e_k )$ : $$ e_k = \frac{u_k}{||u_k||} $$

### Base duale

::: custom-box
Pour une base $\{e_1, e_2, \ldots, e_n\}$ d'un espace vectoriel $E$, la **base duale** $(e^1, e^2, \ldots, e^n)$ est d√©finie par : $$e^i(e_j) = \delta_{ij}$$ o√π $\delta_{ij}$ est le symbole de Kronecker.
:::

La base duale permet de d√©finir des formes lin√©aires et de travailler avec des espaces vectoriels de mani√®re plus abstraite.

### Diagonalisation d'une matrice

::: custom-box
Une matrice carr√©e $A$ est dite **diagonalisable** s'il existe une matrice diagonale $D$ et une matrice inversible $P$ telles que : $$A = PDP^{-1}$$

$D$ est une matrice diagonale contenant les valeurs propres de $A$
:::

#### Diagonalisation d'une matrice sym√©trique r√©elle

Pour les matrices sym√©triques r√©elles, on peut toujours trouver une base orthonormale de vecteurs propres, ce qui permet de les diagonaliser par une matrice orthogonale $Q$ : $$A = QDQ^T$$ o√π $Q$ est une matrice orthogonale.

::: callout-warning
Pour que la propri√©t√© soit v√©rifi√©e, il est essentiel que la matrice soit r√©elle, c'est-√†-dire sans composantes complexes. En effet, la sym√©trie d'une matrice complexe n'implique pas n√©cessairement la propri√©t√© en question
:::

## Statistiques Descriptives Univari√©es

-   **Moyenne** : La moyenne arithm√©tique d'une s√©rie de valeurs $x_1, x_2, \ldots, x_n$ est donn√©e par : $$
    \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
    $$

-   **M√©diane** : La m√©diane est la valeur qui s√©pare la s√©rie en deux parties √©gales. Pour une s√©rie ordonn√©e, si $n$ est impair, la m√©diane est la valeur centrale. Si $n$ est pair, c'est la moyenne des deux valeurs centrales.

-   **Variance** : La variance mesure la dispersion des valeurs autour de la moyenne : $$
    \text{Var}(X) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
    $$

-   **√âcart-type** : L'√©cart-type est la racine carr√©e de la variance : $$
    \sigma = \sqrt{\text{Var}(X)}
    $$

::: callout-note
Plus l'√©cart-type est grand, plus les donn√©es sont dispers√©es autour de la moyenne.
:::

Les statistiques descriptives univari√©es sont essentielles pour r√©sumer et comprendre les caract√©ristiques principales d'une seule variable. Elles sont largement utilis√©es en analyse de donn√©es pour obtenir une vue d'ensemble rapide et efficace. Elles permettent aussi d'identifier rapidement des valeurs extr√™mes.

## Statistiques Descriptives Bivari√©es

### D√©finitions

-   **Covariance** : La covariance entre deux variables $X$ et $Y$ mesure la mani√®re dont deux variables varient ensemble. Elle est donn√©e par deux formules :

1.  Pour un √©chantillon de donn√©es : $$
    \text{Cov}(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
    $$ o√π $x_i$ et $y_i$ sont les valeurs des √©chantillons, et $\bar{x}$ et $\bar{y}$ sont les moyennes des √©chantillons.

2.  Pour des variables al√©atoires : $$
    \text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
    $$ o√π $\mathbb{E}[X]$ et $\mathbb{E}[Y]$ sont les esp√©rances (moyennes th√©oriques) des variables al√©atoires $X$ et $Y$.

-   **R√©gression lin√©aire**:

La r√©gression lin√©aire est une m√©thode statistique qui permet de mod√©liser la relation entre deux variables en ajustant une ligne droite √† un ensemble de donn√©es. L'√©quation de la r√©gression lin√©aire est : $$
y = a + bx
$$ o√π $y$ est la variable d√©pendante, $x$ est la variable ind√©pendante, $a$ est l'ordonn√©e √† l'origine (l'interception de la ligne avec l'axe des ordonn√©es), et $b$ est la pente de la ligne (le taux de changement de $y$ par rapport √† $x$).

### Probabilit√©s

Soit $E$ un ensemble muni d'une tribu $T$. On appelle probabilit√© toute application $P : T \rightarrow \mathbb{R}^+$ telle que : - $P(\emptyset) = 0$ - Si $(A_n)$ est une suite d'√©l√©ments de $T$ deux √† deux disjoints alors : $$P\left(\bigcup_n A_n\right) = \sum_n P(A_n).$$

#### Probabilit√©s conditionnelles

En th√©orie des probabilit√©s, nous nous int√©ressons souvent au comportement d'un al√©a, sachant qu'un autre √©v√©nement est d√©j√† pass√©. C'est ce que nous appelons Les Probabilit√©s Conditionnelles.

Consid√©rant deux √©v√©nements de probabilit√© non nulle, $A$ et $B$, la probabilit√© conditionnelle de $A$ sachant que $B$ est r√©alis√© (couramment dit $A$ sachant $B$) est donn√©e par : $$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

Par commutativit√© de l'intersection, nous avons : $$
P(A \cap B) = P(B \cap A)
$$

En utilisant la formule ci-dessus, nous pouvons √©galement exprimer la probabilit√© conditionnelle de $B$ sachant $A$ : $$
P(B|A) = \frac{P(A|B) \cdot P(B)}{P(A)}
$$

C'est ce que nous appelons \textcolor{red}{LA FORMULE DE BAYES}.

#### Ind√©pendance

Deux √©v√©nements $A$ et $B$ sont dits ind√©pendants si et seulement si : $$P(A \cap B) = P(A) \cdot P(B)$$ En termes courants, deux √©v√©nements sont ind√©pendants si le r√©sultat de l'un n'influence aucunement l'aboutissement de l'autre. Sous condition d'ind√©pendance de $A$ et $B$, la notion de la probabilit√© conditionnelle tombe √† l'eau, car les √©v√©nements √©voluent l'un sans se soucier de l'autre. Ceci se traduit par : $$P(A|B) = P(A)$$ $$P(B|A) = P(B)$$

Notons que si $A$ est ind√©pendant de $B$, il le sera par rapport √† son compl√©mentaire √©galement, et vice versa. En g√©n√©ral, pour une suite $(A_n)$ d'√©v√©nements ind√©pendants, on a : $$P\left(\bigcap A_i\right) = \prod P(A_i) = P(A_1) \cdot \ldots \cdot P(A_n)$$ Cette formule est largement utilis√©e en statistique.

\textcolor{red}{NE PAS CONFONDRE IND√âPENDANCE ET INCOMPATIBILIT√â DES √âV√âNEMENTS}

#### Variable al√©atoire

Une variable al√©atoire est un nombre qui d√©pend du r√©sultat d'une exp√©rience al√©atoire. Chaque ex√©cution de l'exp√©rience g√©n√®re une r√©alisation de la variable al√©atoire.

Math√©matiquement, on d√©finit une variable al√©atoire X comme une fonction $X : T \rightarrow \mathbb{R}$ qui associe √† chaque √©v√©nement s, un r√©el $X(s)$.

Par exemple, dans une queue pour la caisse d'un magasin, le nombre de clients est une variable al√©atoire. La dur√©e de traitement de chaque requ√™te aussi. Remarquons que la premi√®re est un nombre entier. On dit qu'elle est √† support discret. Alors que la deuxi√®me est une dur√©e (un nombre r√©el). On dit qu'elle est √† support continu.

\textcolor{green!70!black}{Qu‚Äôest- ce qui caract√©rise une variable al√©atoire ?}

#### Fonction de r√©partition

Une variable al√©atoire traduit le r√©sultat d'une exp√©rience al√©atoire en nombre r√©el. La fonction de r√©partition transporte le calcul des probabilit√©s concernant les r√©alisations de la variable al√©atoire. C'est la fonction d√©finie par : $$F_X(x) = P(X \leq x)$$ \textcolor{green!70!black}{Propri√©t√©s :}

Pour tout $x$, $0 \leq F_X(x) \leq 1$ $F_X$ est une fonction croissante. $\lim_{x \to -\infty} F_X(x) = 0$ et $\lim_{x \to \infty} F_X(x) = 1$

![](images/fonction_repartition.png){width="50%" fig-align="center"}

#### Probabilit√© ponctuelle / Densit√©

\textcolor{green!70!black}{CAS DISCRET : Probabilit√© ponctuelle}

La probabilit√© ponctuelle est la fonction qui d√©crit les sauts de la fonction de r√©partition : $$P(X = k) = P(X \leq k) - P(X \leq k - 1) = p_k$$ $$\sum p_i = 1$$

\textcolor{green!70!black}{CAS CONTINU : densit√© de probabilit√©}

La densit√© est la fonction qui d√©crit les variations de la fonction de r√©partition : $$f(x) = \frac{dF_X}{dx}(x)$$ $$\int f(x) dx = 1$$

\newpage

### Fondements de probabilit√©s : niveau basique

#### Moments

\textcolor{green!70!black}{ESP√âRANCE}

L'esp√©rance d'une variable al√©atoire est sa valeur attendue. C'est une mesure de localisation de la distribution.

Dans le cas discret : $$E(X) = \sum k \cdot P(X = k)$$ $$k \in X(\Omega)$$ Alors que dans le cas continu : $$E(X) = \int x \cdot f_X(x) \, dx$$ $$x \in X(\Omega)$$

\textcolor{green!70!black}{TH√âOR√àME DE TRANSFERT} $$E(g(X)) = \sum g(k) \cdot P(X = k)$$ $$\forall k \in X(\Omega)$$ $$E(g(X)) = \int g(x) \cdot f_X(x) \, dx$$ $$\forall x \in X(\Omega)$$ \textcolor{green!70!black}{VARIANCE}

La variance d'une variable al√©atoire d√©crit la dispersion de la variable al√©atoire autour de sa valeur moyenne (son esp√©rance).

Elle est d√©finie par : $$Var(X) = E(X^2) - (E(X))^2 = E((x - E(X))^2)$$ Sa racine carr√©e est appel√©e √©cart-type et not√©e g√©n√©ralement : $$\sigma(X) = \sqrt{Var(X)}$$

\newpage

\textcolor{green!70!black}{CENTRAGE ET R√âDUCTION}

Le centrage consiste √† localiser la distribution autour de l'origine et la r√©duction consiste √† normaliser la dispersion. La technique est simple : $$Y = \frac{X - E(X)}{\sigma(X)}$$

\textcolor{green!70!black}{MOMENTS D‚ÄôORDRE r}

Le moment d'ordre r est d√©fini par : $$\mu_r = E(X^r)$$ Le moment centr√© d'ordre r est d√©fini ainsi : $$\muÃÉ_r = E((X - E(X))^r)$$

#### Couples al√©atoires

La fonction conjointe $$F_{X, Y}(x, y) = P(X \leq x \cap Y \leq y)$$ est appel√©e la distribution conjointe de X et Y.

Dans le cas continu, la fonction d√©finie par : $$f_{X, Y}(x, y) = \frac{\partial^2 F_{X, Y}(x, y)}{\partial x \partial y}$$ est la densit√© conjointe du couple (X, Y). On a donc : $$F_{X, Y}(x, y) = \int \int f_{X, Y}(t, u) \, dt \, du, \text{ o√π } -\infty < x, y < +\infty,$$

Dans le cas discret, on d√©finit la fonction de probabilit√© conjointe : $$P(X = x_i, Y = y_j) = p_{ij}$$ On a donc : $$F_{X, Y}(x, y) = \sum \sum p_{ij}, \text{ o√π } x_i \leq x \text{ et } y_j \leq y$$

\textcolor{green!70!black}{LOI MARGINALE}

La loi marginale de X est d√©finie comme suit : $$f_X(x) = \int f_{X, Y}(x, y) \, dy, \text{ o√π } -\infty < x < \infty,$$ dans le cas continu, ou encore : $$f_X(x_i) = \sum p_{ij}, \text{ o√π } j \text{ tel que } y_j \leq y$$

Si X et Y sont ind√©pendants, alors : $$f_{X, Y}(x, y) = f_X(x) \cdot f_Y(y)$$

\textcolor{green!70!black}{COVARIANCE}

La covariance mesure l'intensit√© de la relation lin√©aire entre deux variables al√©atoires X et Y. Elle est d√©finie comme suit : $$Cov(X, Y) = E(XY) - E(X) \cdot E(Y)$$

Si X et Y sont ind√©pendants, alors : $$Cov(X, Y) = 0$$

\textcolor{red}{Il est important de noter que la r√©ciproque n'est pas vraie : la covariance n'implique pas n√©cessairement l'ind√©pendance entre X et Y.}

\newpage

### Propri√©t√©s

\textcolor{green!70!black}{ESP√âRANCE} $$
\mathbb{E}(aX + bY) = a\mathbb{E}(X) + b\mathbb{E}(Y)
$$ $$
\mathbb{E}(a) = a
$$

\textcolor{green!70!black}{VARIANCE} $$
\text{Var}(aX) = a^2\text{Var}(X)
$$ $$
\text{Var}(a) = 0
$$ $$
\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)
$$ $$
\text{Var}(X - Y) = \text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X,Y)
$$

\textcolor{green!70!black}{COVARIANCE} $$
\text{Cov}(X, Y) = \text{Cov}(Y, X)
$$ $$
\text{Cov}(aX + b, cY + d) = ac\text{Cov}(X, Y)
$$ $$
\text{Cov}(aX + bY, U) = a\text{Cov}(X, U) + b\text{Cov}(Y, U)
$$ $$
\text{Cov}(X, cU + dV) = c\text{Cov}(X, U) + d\text{Cov}(X, V)
$$ $$
\text{Cov}(aX + bY, cU + dV) = ac\text{Cov}(X, U) + ad\text{Cov}(X, V) + bc\text{Cov}(Y, U) + bd\text{Cov}(Y, V)
$$

\newpage

### Vecteurs al√©atoires

Pour un vecteur al√©atoire $$(X_1, X_2, \ldots, X_n)$$, l'esp√©rance est toujours lin√©aire. Pour une suite $$(a_i)_{i \in \{1, \ldots, n\}}$$ de r√©els, on a : $$
\mathbb{E}(a_1X_1 + a_2X_2 + \ldots + a_nX_n) = a_1\mathbb{E}(X_1) + a_2\mathbb{E}(X_2) + \ldots + a_n\mathbb{E}(X_n)
$$

Si les variables al√©atoires $$X_1, X_2, \ldots, X_n$$ sont ind√©pendantes, alors la variance de leur somme est √©gale √† la somme de leurs variances individuelles : $$
\text{Var}(X_1 + X_2 + \ldots + X_n) = \text{Var}(X_1) + \ldots + \text{Var}(X_n)
$$

### Lois usuelles

Ces tableaux r√©capitulent les lois usuelles que vous pourrez rencontrer dans diff√©rents cours du master.

```{=tex}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Nom & Notation & $X(\Omega)$ & $P(X = k)$ & $E[X]$ & $V(X)$ \\
\hline
Uniforme & $X \sim U(\{1, 2, \ldots, n\})$ & $\{1, 2, \ldots, n\}$ & $\frac{1}{n}$ & $\frac{n+1}{2}$ & $\frac{n^2-1}{12}$ \\
\hline
Bernouilli & $X \sim B(p), 0 < p < 1$ & $\{0, 1\}$ & $P(X = 1) = p$ & $p$ & $p(1-p)$ \\
& & & $P(X = 0) = 1 - p$ & & \\
\hline
Binomiale & $X \sim B(n, p), 0 < p < 1$ & $\{1, 2, \ldots, n\}$ & $C_k^n p^k (1 - p)^{n-k}$ & $np$ & $np(1-p)$ \\
\hline
G√©om√©trique & $X \sim G(p), 0 < p < 1$ & $\mathbb{N}$ & $p(1-p)^{k}$ & $\frac{1-p}{p}$ & $\frac{1-p}{p^2}$ \\
\hline
Poisson & $X \sim P(\lambda), \lambda > 0$ & $\mathbb{N}$ & $\frac{\lambda^k}{k!}e^{-\lambda}$ & $\lambda$ & $\lambda$ \\
\hline
\end{tabular}
```
```{=tex}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Nom & Notation & $X(\Omega)$ & $f_X(x)$ & $E[X]$ & $V(X)$ \\
\hline
Uniforme & $X \sim U([a, b]), a < b$ & $[a, b]$ & $\frac{1}{b - a}1_{[a, b]}(x)$ & $\frac{a + b}{2}$ & $\frac{(a - b)^2}{12}$ \\
\hline
Exponentielle & $X \sim \mathcal{E}(\lambda), \lambda > 0$ & $\mathbb{R}^+$ & $\lambda e^{-\lambda x}1_{\mathbb{R}^+}(x)$ & $\frac{1}{\lambda}$ & $\frac{1}{\lambda^2}$ \\
& $\mathcal{E}(Œª) = Œ≥(1, Œª)$ & & & & \\
\hline
Normale ou Gaussienne & $X \sim N(m, \sigma^2), \sigma > 0$ & $\mathbb{R}$ & $\frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x - m)^2}{2\sigma^2}\right)$ & $m$ & $\sigma^2$ \\
\hline
Gamma & $X \sim \gamma(\alpha, \theta), \alpha > 0, \theta > 0$ & $\mathbb{R}^+$ & $\frac{\theta^\alpha}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\theta x}1_{\mathbb{R}^+}(x)$ & $\frac{\alpha}{\theta}$ & $\frac{\alpha}{\theta^2}$ \\
& $\Gamma(\alpha) = \int_0^\infty e^{-x}x^{\alpha-1} \, dx$ & & & & \\
\hline
Khi-2 & $X \sim \chi^2(n), n \in \mathbb{N}^+$ & $\mathbb{R}^+$ & $\gamma\left(\frac{n}{2}, \frac{1}{2}\right)$ & $n$ & $2n$ \\
& $Y_1, Y_2, \ldots, Y_n \text{ ind√©pendantes},$ & & & & \\
& $Y_i \sim \mathcal{N}(0, 1), \quad X = \sum_{i=1}^{n} Y_i^2$ & & & & \\
\hline
B√™ta & $X \sim B(\alpha, \theta), \alpha > 0, \theta > 0$ & $[0, 1]$ & $\frac{x^{\alpha-1}(1-x)^{\theta-1}}{B(\alpha, \theta)}1_{[0, 1]}(x)$ & $\frac{\alpha}{\alpha+\theta}$ & $\frac{\alpha\theta}{(\alpha+\theta)^2(\alpha+\theta+1)}$ \\
& $B(\alpha, \theta) = \int_0^1 x^{\alpha-1}(1-x)^{\theta-1} \, dx$ & & & & \\
& $B(\alpha, \theta) = \int_0^1 x^{\alpha-1}(1-x)^{\theta-1} \, dx$ & & & & \\
& $X = \frac{Z}{1 + Z}, \quad Z \sim B'(\alpha, \theta)$ & & & & \\
\hline
B√™ta (prime) & $Z ‚àº B'(Œ±, Œ∏), Œ± > 0, Œ∏ > 0$ & $\mathbb{R}^+$ & $\frac{z^{Œ±-1}}{B(Œ±,Œ∏) \cdot (1+z)^{Œ±+Œ∏}} \cdot 1_{\mathbb{R}^+}(z)$ & $ \frac{Œ±}{Œ∏ - 1}$ & $\frac{Œ±(Œ±+Œ∏-1)}{(Œ∏-1)^2(Œ∏-2)}$ \\
& $X \sim \gamma(\alpha, 1), Y \sim \gamma(\theta, 1), X \perp\!\!\!\perp Y$ & & & & \\
& $Z = \frac{X}{Y}$ & & & $Œ∏ > 1$ & $Œ∏ > 2$ \\
& $B(Œ±, Œ∏) = \int_{0}^{\infty} \frac{x^{\alpha-1}}{(1+x)^{\alpha+\theta}} \, dx$ & & & & \\
\hline
Student T & $T \sim T(n), n \in \mathbb{N}^*$ & $\mathbb{R}$ & $\frac{\left(1 + \frac{t^2}{n}\right)^{-(n+\frac{1}{2})}}{\sqrt{n}\cdot B(\frac{1}2,\frac{n}2)}$ & $0$ & $\frac{n}{n - 2}$ \\
& $X \sim \mathcal{N}(0, 1), Y \sim \chi^2(n), X \perp\!\!\!\perp Y$ & & & & \\
& $T = \frac{X}{\sqrt{Y/n}} $ & & & & $n > 2$ \\
& $T^2/n \sim B'(1/2, n/2)$ & & & & \\
\hline
Fisher & $X \sim F(n, m)$ & $\mathbb{R}^+$ & $\frac{Kx^{n/2-1}}{(m+nx)^{(n+m)/2}}1_{\mathbb{R}^+}(x)$ & $\frac{m}{m-2}$ & $\frac{2m^2(n+m-2)}{n(m-2)^2(m-4)}$ \\
& $N \sim \chi^2(n), n \in \mathbb{N}^*$ & & $K = \frac{n^{\frac{n}{2}} \cdot m^{\frac{m}{2}}}{B\left(\frac{n}{2}, \frac{m}{2}\right)}$ & & \\
& $M \sim \chi^2(m), \quad m \in \mathbb{N}^*$ & & & $m > 2$ & $m > 4$ \\
& $N \perp\!\!\!\perp M, \quad X = \frac{N}{n} / \frac{M}{m}$ & & & & \\
& $\frac{n}{m}X \sim B' \left(\frac{n}{2}, \frac{m}{2}\right)$ & & & & \\
\hline
\end{tabular}
```
\newpage

$$\Gamma(x) = \int_0^{\infty} t^{x-1}e^{-t} \, dt \text{ : d√©signe la fonction Gamma d'Euler}$$

$$B(x, y) = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)} \text{ : d√©signe la fonction B√™ta}$$

Nous allons souvent rencontrer les lois gris√©es dans les Tests statistiques. L√† encore, conna√Ætre les densit√©s ne servirait pas √† grand chose, mais ceci nous √©vitera de parler de lois dont nous ne connaissons pas la t√™te.

\newpage

## Fondements de probabilit√©s

### Vecteurs al√©atoires

\textcolor{green!70!black}{INDEPENDANCE DEUX √Ä DEUX}

Les variables $X_1, \ldots, X_n$ sont deux √† deux ind√©pendantes si et seulement si : $\forall i \neq j, X_i$ et $X_j$ sont ind√©pendantes.

\textcolor{green!70!black}{INDEPENDANCE MUTUELLE}

Les variables $X_1, \ldots, X_n$ sont mutuellement ind√©pendantes si et seulement si : $$P(X_1 = x_1, \ldots, X_n = x_n) = P(X_1 = x_1) \times \ldots \times P(X_n = x_n)$$

\textcolor{red}{L'IND√âPENDANCE MUTUELLE IMPLIQUE L'IND√âPENDANCE DEUX √Ä DEUX. LA R√âCIROQUE EST FAUSSE.}

Si $X_1, \ldots, X_n$ sont mutuellement ind√©pendantes, alors pour toute famille de fonctions r√©elles $f_i$, on a : $(f_1(X_1), \ldots, f_n(X_n))$ sont ind√©pendantes.

\textbf{ESP√âRANCE ET VARIANCE-COVARIANCE}

Soit $X = (X_1, \ldots, X_n)^T$ un vecteur al√©atoire. Dans le cas multidimensionnel, l'esp√©rance scalaire est remplac√©e par un vecteur esp√©rance. $$E(X) = (E(X_1), \ldots, E(X_n))^T$$

La variance unidimensionnelle est remplac√©e par la matrice sym√©trique de variance-covariance. Elle contient les variances en diagonale et les covariances ailleurs. On la note g√©n√©ralement $\Sigma_X$. $$\Sigma_X = \begin{bmatrix}
V(X_1) & \ldots & \text{Cov}(X_1, X_n) \\
\vdots & \ddots & \vdots \\
\text{Cov}(X_n, X_1) & \ldots & V(X_n)
\end{bmatrix}$$

### Notions de convergence

Si l'on pense √† des donn√©es, vues comme r√©alisation de variables al√©atoires $X_1, \ldots, X_n$, il serait int√©ressant de se poser la question de savoir comment √©volue cette suite lorsque $n$ tend vers l'infini.

\textcolor{green!70!black}{Convergence presque s√ªre}

On dit que $(X_n)$ converge presque s√ªrement vers $X$ et on note $X_n \xrightarrow{\text{p.s.}} X$ si et seulement si : $P\left(\lim_{{n\to+\infty}} X_n = X\right) = 1$

\textcolor{green!70!black}{Convergence en probabilit√©}

On dit que $(X_n)$ converge en probabilit√© vers $X$ et on note $X_n \xrightarrow{\text{p}} X$ si et seulement si : $\forall \varepsilon > 0, \quad P(|X_n - X| > \varepsilon) \rightarrow 0$

\textcolor{green!70!black}{Convergence en Loi}

On dit que $(X_n)$ converge en loi vers $X$ et on note $X_n \xrightarrow{\mathcal{L}} X$ si et seulement si : $F_{X_n} \xrightarrow{n \to +\infty} F_X$ O√π $F_X$ d√©note la fonction de r√©partition de $X$.

\textcolor{green!70!black}{Convergence en Moyenne quadratique}

On dit que $(X_n)$ converge en moyenne quadratique vers $X$ et on note $X_n \xrightarrow{m.q.} X$ si et seulement si : $\mathbb{E}((X_n - X)^2) \rightarrow 0$ Cette d√©finition peut se g√©n√©raliser jusqu'√† l'ordre $n$, mais nous n'en aurons pas besoin.

![](images/convergence.png){width="30%" fig-align="center"}

### Loi faible des grands nombres

Soit $X_1, \ldots, X_n$ une suite de variables al√©atoires ind√©pendantes et de m√™me loi telles que : $\mathbb{E}(X_i) = \mu$ et $\text{Var}(X_i) = \sigma^2$ alors :\
$$\frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{p} \mu $$

### Loi forte des grands nombres

Soit $X_1, \ldots, X_n$ une suite de variables al√©atoires ind√©pendantes et de m√™me loi telles que : $\mathbb{E}(X_i) = \mu$ et $\text{Var}(X_i) = \sigma^2$, alors :\
$$\frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{p.s.} \mu$$

### Th√©or√®me Central Limite

Soit $X_1, \ldots, X_n$ une suite de variables al√©atoires ind√©pendantes et de m√™me loi telles que : $\mathbb{E}(X_i) = \mu$ et $\text{Var}(X_i) = \sigma^2$, alors : $$\sqrt{n}\frac{\overline{X}_n - \mu}{\sigma} \xrightarrow{\mathcal{Loi}} \mathcal{N}(0, 1)$$

\newpage

## Statistique inf√©rentielle : niveau basique

### Echantillon / Estimateur

Le point de d√©part est un vecteur (ou un tableau dans le cas multidimensionnel) de donn√©es. Ces donn√©es peuvent √™tre vues comme les r√©alisations $(x_1, x_2, \ldots, x_n)$ d'une variable al√©atoire $X$ qui d√©pend d'un certain param√®tre $\theta$ que nous allons chercher √† estimer. Pour ce faire, nous allons construire un √©chantillon de cette variable. Un √©chantillon $(X_1, X_2, \ldots, X_n)$ est un n-uplet de variables al√©atoires ind√©pendantes qui suivent toutes la m√™me loi (celle de $X$). Un estimateur de $\theta$ est une fonction $\hat{\theta} = f(X_1, X_2, \ldots, X_n)$ de notre √©chantillon, qui poss√®de une loi de probabilit√©. Lorsque l'al√©a est r√©alis√©, $\hat{\theta}(\omega) = f(x_1, x_2, \ldots, x_n)$ est une estimation de $\theta$. Le but de ce cours est de construire le meilleur estimateur possible de $\theta$.

### Estimateur sans biais

Pour que l'estimation soit bonne, il faut que $\hat{\theta}$ soit proche de $\theta$. Comme $\hat{\theta} = f(X_1, X_2, \ldots, X_n)$ est une variable al√©atoire, on ne peut imposer de condition qu'√† sa valeur moyenne.

On d√©finit ainsi le biais : $$b_n(\hat{\theta}, \theta) = \mathbb{E}(\hat{\theta}_n) - \theta$$

Un estimateur est dit sans biais si $b_n(\hat{\theta}, \theta) = 0$, c'est-√†-dire : $$\mathbb{E}(\hat{\theta}_n) = \theta$$

### Estimateur convergent

Un estimateur est dit convergent s'il converge en probabilit√© vers le param√®tre √† estimer : $$\hat{\theta}_n \xrightarrow{P} \theta$$

En pratique, tout estimateur sans biais et dont la variance tend vers 0 est convergent.

### Estimateur optimal

\textcolor{green!70!black}{Qualit√© d‚Äôun estimateur}

La qualit√© d'un estimateur est mesur√©e √† travers son erreur quadratique moyenne d√©finie par : $$EQM(\hat{\theta}_n) = (b_n(\hat{\theta}, \theta))^2 + V(\hat{\theta}_n)$$ Comme nous cherchons tout le temps (presque) des estimateurs sans biais, il reste √† comparer les variances.

Un estimateur ùúÉÃÇ1 est meilleur que ùúÉÃÇ2 si : $$V(\hat{\theta}_1) < V(\hat{\theta}_2)$$

\textcolor{green!70!black}{In√©galit√© de Rao-Cramer/ Efficacit√©}

On d√©finit la quantit√© d'information apport√©e par l'estimateur par : $$
I(\hat{\theta}_n) = -\left( \mathbb{E} \left( \frac{\partial L}{\partial \theta} \right) \right)^2
$$ O√π ùêø(ùë•, ùúÉ) = ‚àè ùëì(ùë•ùëñ) (nous reviendrons sur sa d√©finition)

L'in√©galit√© de Rao-Cramer postule que la variance d'un estimateur ne peut pas aller en del√† d'un certain seuil : $$V(\hat{\theta}_n) \geq \frac{1}{I(\hat{\theta}_n)}$$ Un estimateur est optimal (ou efficace) si sa variance v√©rifie le cas d'√©galit√©.

\newpage

### Construction d'un estimateur

\textcolor{green!70!black}{M√©thode du maximum de vraisemblance}

La m√©thode du maximum de vraisemblance consiste √† affecter $ùúÉ$ la valeur qui maximise la probabilit√© d'observer $(ùë•_1, ùë•_2, ‚Ä¶ , ùë•_ùëõ)$ lorsque l'al√©a du vecteur $(ùëã_1, ùëã_2, ‚Ä¶ , ùëã_ùëõ)$ tombe. Sans trop rentrer dans la th√©orie de la vraisemblance, nous allons pr√©senter un algorithme en cinq √©tapes pour calculer cet estimateur (qui pr√©sente des propri√©t√©s assez s√©duisantes) :

\textcolor{blue}{Etape 1 : Calculer la fonction de vraisemblance}

Dans le cas continu : $$L(\mathbf{x}, \theta) = \prod_{i=1}^{n} f(x_i)$$

Dans le cas discret : $$L(\mathbf{x}, \theta) = \prod_{i=1}^{n} P(X_i = x_i)$$ \textcolor{blue}{Etape 2 : Calculer le log-vraisemblance}

Il s'agit de calculer un maximum, ce qui revient √† d√©river. Il s'agit ici d'un produit de n facteurs, ce qui rend la d√©rivation assez coriace. La fonction logarithmique pr√©sente des propri√©t√©s assez sympas pour faciliter cette t√¢che.

\textcolor{blue}{Etape 3 : Calculer la d√©riv√©e de la log-vraisemblance}

\textcolor{blue}{Etape 4 : R√©soudre l'√©quation d'inconnue $ùúΩ$} $$\frac{\partial (\ln(L))}{\partial \theta} = 0 \Rightarrow \theta = \theta_0$$

\textcolor{blue}{Etape 5 : V√©rifier qu'il s'agit d'un maximum.}

En s'assurant que : $$\frac{\partial^2 (\ln(L))}{\partial \theta^2} < 0$$

\newpage

\textcolor{green!70!black}{M√©thode des moments}

Comme le param√®tre √† estimer intervient dans la densit√© de probabilit√©, les moments th√©oriques sont souvent en fonction de ce param√®tre. Ainsi, la m√©thode des moments consiste √† √©galiser les moments th√©oriques (esp√©rance, variance) √† leurs √©quivalents empiriques et √† en d√©gager une estimation ponctuelle.

En pratique, il faut r√©soudre l'(les) √©quation(s) : $$\mathbb{E}(X) = \overline{X} \text{ et } \text{Var}(X) = S_n^2$$ avec : $$\overline{X} = \frac{1}{n} \sum_{i=1}^{n} X_i \hspace{2cm}
 S_n^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2$$

\textcolor{green!70!black}{M√©thode des moindres carr√©s ordinaires}

Lorsqu'il s'agit de prendre une mesure ùúÉ avec un appareil dot√© d'une impr√©cision $ùúÄ$, alors le probl√®me d'estimation peut s'√©crire : $ùëã = ùúÉ + ùúÄ$. La m√©thode des moindres-carr√©s ordinaires consiste √† trouver le param√®tre $ùúÉ$ qui minimise la somme des carr√©es des erreurs : $$ùúÉ_{ùëÄùê∂ùëÇ} = \arg\min \left( \sum_{i=0}^n \varepsilon_i^2 \right) = \arg\min \left( \sum_{i=0}^n (X_i - \theta)^2 \right)$$

### Intervalles de confiance

Un intervalle de confiance \[$A$, $B$\] de niveau $1 - \alpha$ est un intervalle al√©atoire qui a la probabilit√© $1 - \alpha$ de contenir le param√®tre √† estimer $\theta$. Formellement, on √©crit : $P (t_1 (\theta) \leq f(X_1, \ldots, X_n) \leq t_2 (\theta)) = P(A \leq \theta \leq B) = 1 - \alpha$

\newpage

### Test d'hypoth√®ses

Dans le cadre d'un test d'hypoth√®se, nous cherchons √† faire valoir une hypoth√®se en d√©pit d'une autre, qui lui est contradictoire.

On appellera la premi√®re (celle dont le rejet √† tort sera le plus pr√©judiciable) ¬´ Hypoth√®se nulle ¬ª et la deuxi√®me ¬´ Hypoth√®se alternative ¬ª.

![](images/test_hypo.png){width="50%" fig-align="center"}

Les calculs qui se cachent derri√®re le choix de l'hypoth√®se √† garder sont compliqu√©s. Mais BONNE NOUVELLE, la machine fera tour √† notre place. Il suffit juste de suivre correctement la m√©thode :

\textcolor{blue}{Etape 1 : Choisir judicieusement les hypoth√®ses √† √©valuer et fixer le risque $ùõº$}\
\textcolor{blue}{Etape 2 : Choisir le test adapt√© √† la proc√©dure}\
\textcolor{blue}{Etape 3 : Rentrer la commande correspondante sur R et ex√©cuter}\
\textcolor{blue}{Etape 4 : Lire dans les sorties la p-value. si elle est sup√©rieure √† Œ± on accepte H0. Si elle lui est inf√©rieure, on rejette H0}

\newpage

### Construction d'intervalles de confiance

Les intervalles de confiance sont des outils essentiels en statistique pour estimer des param√®tres inconnus tout en mesurant l'incertitude associ√©e √† cette estimation. Ci-dessous, vous trouverez un tableau pr√©sentant la construction des intervalles de confiance pour diff√©rents param√®tres.

```{=tex}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{images/intervalles_conf.png}
\end{figure}
```
# Economie

## Concepts de base

### Macro√©conomie

![](images/macroeconomics.png){fig-align="center" width="73"}

La macro√©conomie est l'√©tude √©conomique d'un syst√®me ou de ph√©nom√®nes √† un niveau global de l'√©conomie.

### Micro√©conomie

La micro√©conomie se concentre sur l'observation et l'analyse des interactions √† petite √©chelle.

### Bien √©conomique

"Chose utile √† satisfaire un besoin, il faut que le bien soit disponible et en quantit√© limit√©e.

Un bien non √©conomique est un bien qui s'obtient gratuitement, comme l'oxyg√®ne, contrairement √† un bien √©conomique qui s'obtient en payant."

### Agent √©conomique

"Un agent √©conomique est un individu ou un groupe d'individus constituant un centre de d√©cision √©conomique ind√©pendant."

### March√©

![](images/economie-circulaire.png){fig-align="center" width="73"}

"Le march√© c'est une institution sociale qui permet l'√©change entre l'offre et la demande."

### Asym√©trie d'information

"L'asym√©trie d'information concerne les situations o√π les agents d'un march√© ne poss√®dent pas de la m√™me information sur un produit que ce soit au sujet de ses qualit√©s ou de ses d√©fauts"

### Externalit√©

L'externalit√© d√©signe la cons√©quence (positive ou n√©gative) d'une activit√© d'un agent √©conomique sur un autre, sans qu'aucun des deux ne re√ßoive ou ne paye une compensation pour cet effet.

### Concurrence Pure et Parfaite

La **CPP** repose sur cinq fondements :

-   L'Atomicit√© du march√©

Existence d'un grand nombre d'agent √©conomique sur le march√©, √† tel un point que ni l'offre ni la demande ne peut exercer une action quelconque sur la production et les prix ;

-   L'Homog√©n√©it√© des produits

La pr√©f√©rence d'un produit √† un autre du point de vue de l'acheteur se fait uniquement selon son prix ;

-   Libre entr√©e et sortie sur le march√©

Aucune firme ne peut s'opposer √† l'arriv√©e d'un concurrent sur le march√©, tout le monde est libre de l'int√©grer ;

-   Libre circulation des facteurs de production

Les facteurs de production (capital et travail) doivent √™tre libre de se d√©placer librement sans obstacle d'une industrie √† l'autre ;

-   La transparence de l'information

Offreurs et demandeurs sont parfaitement conscient des caract√©ristiques et prix des produits.

### Monopole

"Le monopole est une situation dans un march√© o√π un vendeur fait face aux multitudes vendeurs."

### Segmentation de march√©

"La segmentation de march√© est un d√©coupage du march√© en groupes homog√®nes selon des crit√®res sp√©cifiques, que ce soit des crit√®res d√©mographiques ou bien g√©o-graphiques."

### Discrimination par les prix

"La discrimination par les prix est le pouvoir de pratiquer des prix diff√©rents pour un m√™me produit, peut s'appliquer sur la quantit√© ou bien selon la segmentation du march√©."

### Utilit√©

"L'utilit√© mesure le bien-√™tre li√©e √† la consommation d'un bien."

### Actualisation

"L'Actualisation est un calcul permettant de transformer une valeur future en une valeur pr√©sente."

Que vaut aujourd'hui les X euros que j'aurais demain ?

$$
V_a = \frac{V_f}{(1+i)^t}
$$

$$
V_a : Valeur\ Actuelle
$$

$$
V_f : Valeur\ future
$$ $$
i : Taux\ sans\ risque\\ 
$$

$$
t : Temps
$$

### Probl√®mes macro√©conomiques

Il existe 4 grands probl√®mes macro√©conomiques :

-   Crises et r√©cessions Ralentissement et/ou r√©gression de l'activit√© √©conomique ;

-   Inflations Augmentation g√©n√©rale et durable du niveau des prix entra√Ænant une perte du pouvoir d'achat de la monnaie ;

-   Ch√¥mage Inactivit√© due au manque de travail ;

-   Probl√®me de l'√©quilibre ext√©rieur Quand les importations sont plus importantes que les exportations, la balance commerciale est d√©s√©quilibr√©e.

## La dissertation en √©conomie

*"On tient tout d'abord √† remercier l'enseignant chercheur (en Philosophie √©conomique, Th√©ories √©conomiques de la justice, Redistribution des revenus, Economie sociale, Economie publique), monsieur **Jean-S√©bastien Gharbi** pour cette rubrique d'aide √† la dissertation."*

![](images/dissertation.png){fig-align="center" width="320"}

On pense souvent que la dissertation en √©conomie est un exercice difficile et qui r√©compense mal le travail. C'est totalement faux. La dissertation est un exercice dans lequel il est facile d'obtenir la moyenne, et m√™me (avec un peu d'entra√Ænement) d'obtenir syst√©matiquement de tr√®s bonnes notes. C'est un exercice relativement facile parce que c'est un exercice en tr√®s grande partie formel¬†: tout est une question de m√©thode.

Faire une dissertation, c'est *montrer que vous √™tes capable d'utiliser et de r√©organiser vos connaissances pour r√©pondre √† une question de mani√®re argument√©e (c'est-√†-dire sous la forme d'un raisonnement)*. Autrement dit¬†:

-   ***Une dissertation n'est pas une question de cours.***

La premi√®re chose √† faire, c'est de diff√©rencier question de cours et dissertation (qui sont souvent confondues). Une question de cours demande simplement de r√©citer un cours. Si vous ne faites que r√©citer votre cours dans un exercice de dissertation, vous aurez une mauvaise note. Pourquoi¬†? Parce que *l'exercice de dissertation suppose de montrer que vous √™tes capable d'utiliser et de r√©organiser vos connaissances* (dans un temps limit√©) -- pas seulement de r√©citer une le√ßon apprise plus ou moins par c≈ìur. Comme la question de cours, la dissertation suppose donc que vous savez des choses sur le sujet, mais il est important de comprendre que la dissertation porte tout autant sur votre aptitude √† organiser vos id√©es que sur vos connaissances.

-   ***Dans une dissertation, la r√©ponse donn√©e n'est pas importante !***

Une dissertation consiste toujours √† r√©pondre √† une question. Les √©tudiants pensent parfois qu'il y a une ¬´¬†bonne¬†¬ª r√©ponse √† la question pos√©e -- qu'il s'agirait de trouver. D'ailleurs, cela contribue √† l'id√©e (fausse) que la dissertation est un exercice al√©atoire¬†: si vous ne trouvez pas la bonne r√©ponse, vous avez perdu. Cela aussi est faux¬†: *il n'y a (en g√©n√©ral) pas de ¬´¬†bonne¬†¬ª r√©ponse √† la question pos√©e par la dissertation*. L'exercice de dissertation vient de la philosophie. Pensez-vous s√©rieusement que l'on puisse demander √† un √©tudiant (ou √† un professeur, d'ailleurs) de r√©gler de fa√ßon d√©finitive un d√©bat philosophique qui a donn√© lieu √† des controverses pendant des si√®cles en trois, quatre ou m√™me sept heures¬†? La r√©ponse √©vidente est ¬´¬†non¬†¬ª.

-   ***Dans une dissertation, le plus important c'est l'argumentation !***

Si on ne s'int√©resse pas √† la r√©ponse donn√©e. C'est tout simplement, parce que *ce qui int√©resse votre lecteur, c'est la mani√®re dont vous r√©pondez* : votre aptitude √† utiliser vos connaissances de mani√®re argumentative pour d√©fendre une conclusion. Sur le principe, il serait donc possible de d√©fendre une conclusion choquante ou m√™me offensante dans une dissertation, pour la bonne raison qu'on n'√©value pas la r√©ponse que vous donnez, mais la mani√®re dont vous amenez votre r√©ponse. Votre r√©ponse, √† la limite, on ne s'y int√©resse pas. Une fois cela dit, il est assez √©vident qu'il est beaucoup plus facile de d√©fendre une position mod√©r√©e et consensuelle, qu'une position offensante pour de nombreuses personnes. C'est la raison pour laquelle, il n'est pas du tout conseill√© de chercher la provocation gratuite dans une dissertation.

[**Comment on fait une dissertation ?**]{.underline}

![](images/question-02.png){fig-align="center" width="174"}

### Analyse du sujet

L'analyse du sujet est la premi√®re √©tape de la dissertation et l'une des plus importantes. Une dissertation se pr√©sente sous la forme d'un sujet. Il faut isoler la ou les deux notions principales du sujet.

Il y a quatre grands types de sujets¬†: les sujets ne contenant qu'une seule notion (ex¬†: ¬´¬†Les discriminations en France¬†¬ª), les couples de notions (ex¬†: ¬´¬†Capitalisme et d√©mocratie¬†¬ª), les citations (ex¬†: ¬´¬†*Le syst√®me de production capitaliste est une d√©mocratie √©conomique dans laquelle chaque sou donne un droit de vote. Les consommateurs constituent le peuple souverain*¬†¬ª, Ludwig von Mises) ou une question (ex¬†: ¬´¬†La croissance √©conomique s'oppose-t-elle √† la pr√©servation de l'environnement¬†?¬†¬ª).

Dans tous les cas, l'objectif est d'arriver √† une question (donc les sujets les plus simples √† traiter √† ce stade, ce sont les questions¬†: ils vous donnent imm√©diatement le probl√®me √† traiter). Mais la premi√®re chose √† faire (m√™me quand on a d√©j√† la question), c'est de trouver le couple de notions impliqu√©es dans le sujet. Souvent, c'est absolument √©vident, mais parfois il faut un peu chercher.

Il faut √©viter √† tout prix de faire un expos√© quand on attend de vous une dissertation. Un hors-sujet, c'est de ne pas traiter le bon sujet. Si vous r√©pondez de mani√®re factuelle √† un sujet de dissertation, vous faites pire¬†: vous faites un hors-exercice.

### Recherche des id√©es

Une fois qu'on a identifi√© un couple de notions, il faut (au brouillon) essayer de faire la liste des √©l√©ments du cours qui relient les deux notions. Il est important de ne noter que les √©l√©ments qui relient les deux notions (pour ne pas risquer de se perdre dans des √©l√©ments qui concernent seulement une seule des deux notions). Sur chacune des notions que vous aurez √† traiter en dissertation, on a √©crit des livres entiers. Il est impossible de tout dire dessus dans une dissertation. On se limite donc √† ce qui relie les deux notions de notre sujet. √âvidemment, si un √©l√©ment pertinent vous vient en t√™te et qu'il ne se trouve pas dans votre cours, n'h√©sitez pas √† le noter. Si cet √©l√©ment peut √™tre utilis√© dans votre raisonnement, m√™me comme exemple, ce sera un plus indiscutable.

Dans un premier temps, on note tout ce qui se pr√©sente √† l'esprit. Ce n'est que dans un deuxi√®me temps, quand on a un certain nombre d'√©l√©ments que l'on se pose la question¬†: ¬´¬†Est-ce qu'il y a une mani√®re qui saute aux yeux de relier tous ces √©l√©ments en r√©pondant √† la question (si elle a √©t√© pos√©e de mani√®re directe) ou pour r√©pondre √† une question comprenant le couple de notions (si la question n'a pas √©t√© formul√©e dans le sujet)¬†? ¬ª. Si la r√©ponse est positive, on a trouv√© la question qui structurera notre devoir. Si ce n'est pas le cas, il faut essayer de trouver une question qui relie le plus grand nombre des √©l√©ments que l'on a not√© sur son brouillon¬†-- et donc laisser de c√¥t√© les √©l√©ments qui ne servent pas. Il arrive souvent qu'une partie des √©l√©ments que l'on note sur son brouillon ne soit pas utilis√©e dans le devoir. Bref, si notre sujet est un couple de notions ou une citation, il faut que l'on arrive √† une question. √âvidemment, quand notre sujet est d√©j√† une question, on n'a pas autant de marge de man≈ìuvre, mais en v√©rit√©, si on vous pose une question pr√©cise, c'est que vous avez les √©l√©ments pour y r√©pondre dans le cours (donc la diff√©rence n'est pas tr√®s importante).

### Mise en √©vidence d'un probl√®me

Puisqu'elle ne doit pas √™tre une question de fait, la question qui relie le plus d'√©l√©ments possibles parmi ceux qui associent les deux notions dans votre cours doit √™tre une question conceptuelle. Pour le dire autrement, cette question doit √™tre un probl√®me (on parle souvent de ¬´¬†probl√©matique¬†¬ª pour d√©signer ce probl√®me dans le cadre d'une dissertation). Qu'est-ce qu'un probl√®me¬†? C'est une question qui met en tension deux concepts et qui analyse les diff√©rents aspects de leur relation (conceptuelle).

Souvent les √©tudiants ont peur de ne pas trouver le ¬´¬†bon¬†¬ª probl√®me. Pourtant, si on suit la m√©thode de dissertation, il n'y a pas de risque de se tromper. En effet, on ne doit pas choisir un probl√®me d'abord (sans savoir si on a de quoi le traiter) et le traiter ensuite. Vous aurez not√© qu'on proc√®de exactement dans le sens inverse¬†: on voit √† quelle question on peut r√©pondre avec les √©l√©ments qu'on a sur son brouillon et on pose pr√©cis√©ment la question √† laquelle on sait qu'on peut r√©pondre.

### Construction du plan

Pour la m√™me raison qu'au-dessus, la construction du plan ne doit pas √™tre tr√®s difficile¬†: il s'agit de rassembler les diff√©rents √©l√©ments qui permettent de r√©pondre √† la question (au probl√®me que l'on va poser) de fa√ßon √† y apporter une r√©ponse.

On va apporter la r√©ponse que les √©l√©ments disponibles nous permettent d'atteindre. Il y a une seule contrainte¬†: votre plan doit √™tre suffisamment d√©taill√©. Comme l'objectif de la dissertation, c'est de montrer que vous √™tes capable d'utiliser et de r√©organiser vos connaissances. Si vous ne faites que deux parties sans sous-parties √† l'int√©rieur (il n'y aurait donc qu'une seule articulation logique), on trouvera que vous n'avez pas assez structur√© votre devoir. Le d√©coupage minimal, c'est d'avoir quatre √©l√©ments (en g√©n√©ral, on fait deux grandes parties avec deux sous-parties chacune, donc on a trois articulations).

Vos parties et vos sous-parties doivent correspondre √† des √©tapes de votre raisonnement (on dit souvent qu'il faut une id√©e par sous-partie), donc votre plan doit donner la structure du raisonnement gr√¢ce auquel vous allez r√©pondre √† la question pos√©e. Le plan (qui est annonc√© √† la fin de l'introduction et qui doit √™tre apparent dans le devoir, nous reviendrons sur ce point un peu plus loin) doit permettre de comprendre la structure de votre devoir d'un coup d'≈ìil -- simplement en lisant les titres.

Un √©l√©ment qui permet de savoir si votre plan est bon, c'est de se demander si √† la fin de la premi√®re partie, on est arriv√© √† un √©tat de la r√©flexion diff√©rent de celui de la fin de l'introduction.

Qu'est-ce que la premi√®re partie a permis de comprendre¬†? Et est-ce que la seconde partie apporte quelque chose d'autre¬†? Si chaque partie repr√©sente une √©tape dans un raisonnement et que votre devoir complet est donc un raisonnement, votre plan est forc√©ment bon¬†: vu que c'est pr√©cis√©ment ce qu'on attend de vous.

Il ne faut jamais faire deux sous-parties dans une partie sous la forme d'une seule phrase coup√©e par des points de suspension (ex¬†: ¬´¬†A) L'organisation scientifique du travail a permis la croissance des trente glorieuses...¬†¬ª, ¬´¬†B) mais, elle a aussi eu des cons√©quences n√©gatives, notamment sur le plan social¬†¬ª). En effet, cela revient √† pointer du doigt que vous op√©rez une coupure arbitraire (donc que vous n'articulez pas de mani√®re assez nette les diff√©rentes parties de votre devoir). Sur le principe, les deux sous-parties sont reli√©es par les points de suspension et donc ne forment qu'une partie sans coupure. Pr√©f√©rez toujours les titres qui se succ√®dent sans √™tre grammaticalement li√©s les uns aux autres. Dans l'exemple ci-dessus, il suffit de faire deux phrases pour d√©couper les deux id√©es. Si on ne peut pas couper grammaticalement les deux titres, c'est la preuve que l'articulation pose probl√®me.

Dans l'id√©al, un plan est √©quilibr√©¬†: chaque partie comprend le m√™me nombre de sous-parties que l'autre et elles font √† peu pr√®s la m√™me longueur. Et si vous faites plus de sous-parties dans une partie que dans l'autre, il faut que les sous-parties soient un peu plus longues dans la partie qui contient moins de sous-parties. Remarquez que si vous faites un plan avec deux parties, deux sous-parties, ce dernier probl√®me ne se pose pas.

Un point important et souvent totalement n√©glig√© par les √©tudiants¬†: m√™me quand c'est tentant, *on ne fait jamais de plan centr√© sur les auteurs*. Un plan par auteurs conduit tr√®s souvent √† suivre l'ordre chronologique et √† pr√©senter les positions des auteurs sans les confronter r√©ellement les unes aux autres . Ce qui doit structurer le plan, ce sont les concepts (c'est-√†-dire les notions qui nous avaient permis de construire le probl√®me √† r√©soudre).

En r√©alit√©, il n'est pas rare qu'on suive plus ou moins l'ordre chronologie, mais il est essentiel de se focaliser sur les concepts, et pas sur les auteurs. Pourquoi¬†? Parce que l'encha√Ænement ou l'opposition de concepts constitue un raisonnement (ce que vous devez faire¬†!), alors que l'encha√Ænement ou l'opposition d'auteurs constitue un expos√© (ce que vous ne devez pas faire¬†!). En r√©alit√©, c'est assez facile √† faire il suffit de s'interdire de mentionner le nom des auteurs dans les titres de partie ou de sous-partie.

Vu que l'objectif du plan, c'est de r√©pondre √† une question qui met les deux notions du sujet en relation, *les parties (ou les sous-parties) qui se focalisent sur une seule des deux notions sont √† √©viter √† tout prix¬†: elles sont simplement hors-sujet*. Sur un sujet comme ¬´¬†capitalisme et d√©mocratie¬†¬ª, le plan ¬´¬†premi√®re partie¬†: capitalisme¬†¬ª, ¬´¬†deuxi√®me partie¬†: d√©mocratie¬†¬ª est parmi les pires possibles.

**Jusqu'√† pr√©sent, nous n'avons encore rien √©crit sur la copie elle-m√™me.** Nous n'avons travaill√© que sur le brouillon. Nous avons deux notions cl√©s, un probl√®me et un plan. Ce sont les √©l√©ments fondamentaux du devoir. Il faut √† pr√©sent passer √† la r√©daction. Nous allons nous int√©resser d'abord √† l'introduction.

### La r√©daction

[**(Introduction, Conclusion, D√©veloppement)**]{.underline}

**L'introduction** est la partie la plus importante de la dissertation. Elle permet de savoir pourquoi le probl√®me se pose, comment il se pose et comment il va √™tre r√©solu. A quoi sert l'introduction¬†?

*Le r√¥le de l'introduction, sa raison d'√™tre, c'est de construire et d'√©noncer le probl√®me (la probl√©matique)* auquel le reste du devoir va r√©pondre. Il ne suffit donc pas de poser la question (pour cela deux lignes suffiraient) et de commencer le d√©veloppement. L'introduction, comme son nom le dit tr√®s bien, va introduire le probl√®me, c'est-√†-dire qu'elle va nous y amener, rapidement, certes, mais en plusieurs √©tapes tr√®s codifi√©es.

Une introduction de dissertation comprend obligatoirement (au minimum) cinq √©l√©ments¬†: une accroche, une d√©finition des termes du sujets, la construction du probl√®me, l'√©nonc√© du probl√®me et l'annonce du plan. Comme une introduction de dissertation fait entre 20 lignes et une page et demie (grand maximum), il faut √™tre efficace.

-   ***L'accroche***

***Une introduction de dissertation suit des r√®gles assez rigides. Elle commence toujours par une accroche***.

*Une ¬´¬†accroche¬†¬ª, c'est une phrase ou deux qui vont contenir la ou les deux notion(s) du sujet*. Son r√¥le est d'amener par √©tapes le lecteur vers la question que vous allez poser. Elle sert donc d'introduction √† l'introduction. Une accroche peut √™tre une citation (il y en a toujours dans un cours) ou un fait r√©cent (le ch√¥mage a-t-il baiss√© r√©cemment¬†? Un candidat √† l'√©lection pr√©sidentielle a-t-il dit qu'il fallait juger sa politique en fonction de son impact sur le niveau de ch√¥mage¬†?). Si on n'a pas de citation ou de fait relevant de l'actualit√©, on peut amener le sujet de fa√ßon plus habituelle.

Il est important d'√©viter un certain nombre de formules toutes faites et souvent utilis√©es comme ¬´¬†De tous temps...¬†¬ª, ¬´¬†De tous temps, les hommes...¬†¬ª ou encore les affirmations tr√®s g√©n√©rales (et que vous ne justifierez pas) comme¬†: ¬´¬†Le ch√¥mage est un ph√©nom√®ne √©conomique important, c'est pourquoi il faut l'√©tudier¬†¬ª. Le d√©faut de tous ces d√©buts d'accroche, c'est qu'ils peuvent servir pour n'importe quel sujet et que cela se voit.

On met une accroche parce que cela permet de mentionner les termes du sujet sans commencer directement par une d√©finition -- ce qui constitue l'√©tape suivante.

-   **D√©finition des termes du sujet**

L'accroche a introduit les notions, mais sans les d√©finir -- comme si tout le monde savait pr√©cis√©ment de quoi il s'agit (ce qui n'est pas si surprenant, on ne passe pas son temps √† d√©finir tous les mots qu'on utilise). Mais, pour utiliser les deux notions du sujet de fa√ßon un peu plus pr√©cise, il faut les d√©finir. Les d√©finitions que l'on va donner dans une introduction n'ont pas pour objectif de d√©finir les notions de mani√®re exhaustive ou dans l'absolu. Elles doivent permettre de comprendre le lien (ou l'opposition) entre les deux notions et orienter l'introduction de fa√ßon √† ce que l'on puisse construire le probl√®me -- avant de l'√©noncer (autrement dit, elles doivent ouvrir la voie aux deux √©tapes suivantes de l'introduction).

Du coup, les d√©finitions que l'on va donner vont d√©pendre du probl√®me que l'on souhaite atteindre.

-   **Construction du probl√®me**

Comme on sait √† quelle question on doit arriver (que cette question nous ait √©t√© donn√©e par le sujet ou que ce soit la question √† laquelle on est le mieux arm√© pour apporter une r√©ponse), il ne va pas √™tre difficile de passer des d√©finitions au probl√®me. Cela suppose juste de montrer qu'avec les d√©finitions que l'on vient de donner, il y a une question se pose avec force.

Encore une fois, cela peut sembler tr√®s artificiel (et √ßa l'est). Toutefois, l'int√©r√™t de cet aspect artificiel, c'est qu'il nous garantit que l'on ne va pas se perdre en chemin. Quand on fait une dissertation, on ne cherche pas son chemin¬†: on sait o√π on va et on ne fait qu'expliquer pourquoi on y va. Le sujet que l'on construit ne tombe pas du ciel, il vient de notre cours. Les d√©finitions ne tombent pas du ciel, elles donnent les √©l√©ments qui vont nous permettre de poser la question √† laquelle on sait d√©j√† comment on va r√©pondre. Bref, l'√©tape de construction du probl√®me est importante parce qu'elle montre que vous avez des aptitudes pour vous faire comprendre √† l'√©crit (et il ne faut surtout pas la n√©gliger), mais elle n'est pas une √©tape difficile ou magique.

Vous pourriez √™tre surpris que l'on construise le sujet, alors qu'il nous est parfois donn√© sous forme de question (dans les autres cas, on comprend mieux pourquoi il est n√©cessaire de construire le probl√®me). En fait, c'est une mani√®re de montrer que vous √™tes capable de vous approprier le sujet. Vous ne traitez pas le sujet parce qu'on vous l'a donn√© (m√™me si vous c'est une des raisons pour lesquelles vous faites une dissertation), mais parce que vous comprenez pourquoi la question se pose. Et comment mieux montrer qu'on comprend un probl√®me qu'en montrant en quoi il est probl√©matique ? Autrement dit, m√™me quand votre sujet a la forme d'une question, vous devez passer par l'√©tape de construction du sujet dans l'introduction.

-   **Enonc√© du probl√®me**

L'√©nonc√© du probl√®me doit prendre la forme d'une question. Il est le point final de l'√©tape juste pr√©c√©dente. Une fois qu'on a les √©l√©ments qui permettent de comprendre que le probl√®me se pose, il faut explicitement exprimer le probl√®me lui-m√™me. *On exprime toujours le probl√®me sous la forme d'une question (parce que c'est une mani√®re de montrer qu'il appelle une r√©ponse) et d'une question unique*. Poser deux, trois ou quatre questions ce serait soit redire plusieurs fois la m√™me chose (et si votre premi√®re question est claire, c'est inutile), soit poser (volontairement ou pas) plusieurs questions diff√©rentes. Or, vous ne pourrez pas r√©pondre convenablement et dans les r√®gles de la dissertation √† plusieurs questions en un seul devoir. Vous devrez donc choisir entre ne pas traiter certaines des questions que vous avez explicitement pos√©es (et dans ce cas pourquoi les poser explicitement) ou essayer de les traiter toutes (ce qui vous conduira √† un devoir dont la ligne directrice sera au mieux difficile √† suivre, au pire inexistante). Si on se rappelle du c√¥t√© formel et rh√©torique d'une dissertation, on comprend qu'il ne faut poser qu'une seule question¬†: celle √† laquelle vous apportez une r√©ponse.

Lorsque le sujet est une question, faut-il r√©p√©ter mot pour mot le sujet comme √©nonc√© du probl√®me¬†? Il y a deux √©coles¬†: la premi√®re dit qu'il faut reformuler la question pour montrer que vous la comprenez. Ainsi un sujet comme ¬´¬†Les d√©penses publiques permettent-elles de r√©duire le ch√¥mage¬†?¬†¬ª, on pourrait proposer une probl√©matique comme ¬´¬†les d√©penses publiques sont-elles efficaces √† court et √† long terme pour lutter contre le ch√¥mage¬†?¬†¬ª.

Si vous faites correctement votre travail de d√©finition des termes et de construction du sujet (dans les deux √©tapes pr√©c√©dentes), je pense qu'aucun correcteur ne vous reprochera de reprendre le sujet mot pour mot dans votre √©nonc√© du probl√®me. Ce qui pose probl√®me pour les partisans de la premi√®re fa√ßon de faire, c'est quand on peut se demander si l'√©tudiant comprend que la question qu'il pose est un probl√®me conceptuel, c'est-√†-dire qui vient d'une tension entre deux notions. Dans une introduction qui remplit correctement son r√¥le de construction du probl√®me, le fait de r√©p√©ter le sujet mot pour mot n'est pas un souci.

-   **Annonce du plan**

Une introduction doit toujours se terminer par une annonce du plan (ce n'est pas une option, c'est une obligation). L'annonce de plan dit √† votre lecteur comment vous allez r√©pondre au probl√®me que vous venez de poser. Dans une dissertation, on ne joue pas sur le suspens. On ne cherche pas √† surprendre son correcteur. Il faut donc annoncer le plan de mani√®re √† ce qu'il comprenne que vous allez r√©pondre au probl√®me pos√© par un raisonnement et qu'il comprenne aussi quels vont √™tre les principales √©tapes de votre raisonnement (c'est-√†-dire de votre devoir).

Vous allez donc annoncer vos (deux ou trois) grandes parties. Il est conseill√© fortement d'utiliser les formules (un peu lourdes en termes de style, mais tr√®s claires) ¬´¬†dans une premi√®re partie, nous montrerons que...¬†¬ª, puis ¬´¬†dans une deuxi√®me partie, nous verrons que ...¬†¬ª. Quand vous ne le faites pas, il arrive trop souvent que votre lecteur ne sache pas si vous allez faire formellement deux ou trois parties -- pour peu que vous utilisiez des mots comme ¬´¬†et¬†¬ª, ¬´¬†puis¬†¬ª ou ¬´¬†ensuite¬†¬ª, qui peuvent aussi bien marquer des √©tapes √† l'int√©rieur d'une grande partie que le passage d'une partie √† une autre.

**La conclusion**

**Vous pourriez √™tre surpris de voir la conclusion arriver aussi t√¥t dans le devoir.** La raison, c'est qu'il est inconcevable de ne pas r√©pondre √† la question pos√©e en introduction -- si vous ne r√©pondez pas le devoir n'aura, litt√©ralement, servi √† rien. Or, il est √©vident qu'en partiel, on est souvent pris par le temps. On r√©dige donc la conclusion juste apr√®s avoir r√©dig√© l'introduction au brouillon (on la r√©dige aussi au brouillon, d'ailleurs). Comme √ßa si on est pris par le temps, on pourra recopier la conclusion d√©j√† pr√™te avant de rendre le devoir. S'il faut couper quelque chose en raison du temps limit√© de l'√©preuve, il vaut mieux couper un bout du d√©veloppement que rendre une dissertation sans conclusion.

La premi√®re phrase de votre conclusion doit apporter la r√©ponse √† la question que vous avez pos√©e en introduction. Elle doit le faire de fa√ßon absolument claire et donc il est conseill√© de reprendre exactement la question en la tournant en une phrase affirmative ou en une phrase n√©gative selon votre r√©ponse. *Le r√¥le de la conclusion, c'est de r√©pondre √† la question*. Il ne faut pas qu'on relise la conclusion en se demandant quelle √©tait la r√©ponse -- et m√™me en se demandant si une r√©ponse a √©t√© donn√©e. Cela ne vous emp√™che pas de donner une r√©ponse nuanc√©e, mais il faut une r√©ponse claire.

Une conclusion de dissertation ne r√©sume pas le devoir (on vient de le lire, c'est tout √† fait inutile). Une conclusion n'introduit jamais un √©l√©ment qui n'a pas √©t√© abord√© dans le devoir, mais qui aurait pu y √™tre discut√©. Si jamais votre correcteur n'a pas vu que vous avez oubli√© de parler de quelque chose d'important, vous n'allez tout de m√™me pas lui dire qu'il manque quelque chose dans votre devoir (chacun son boulot). La dissertation est un exercice de rh√©torique, votre objectif, c'est de convaincre votre lecteur¬†: ce n'est pas √† vous de dire qu'il manque quelque chose, m√™me si vous le savez.

On conseille parfois de finir sa dissertation sur une ouverture. Une ouverture est un nouveau probl√®me qui se pose une fois que vous avez r√©pondu au probl√®me de votre devoir. Cela revient √† sugg√©rer une autre dissertation possible une fois qu'on consid√®re votre r√©ponse comme accept√©e. Trop souvent, les √©tudiants finissent leurs devoirs de mani√®re particuli√®rement maladroite parce qu'ils ne comprennent pas ce qu'est une ouverture. Mon conseil est d'√©viter de faire une ouverture, au moins au d√©but¬†: ce n'est pas une obligation et cela peut donner une tr√®s mauvaise impression finale.

**La r√©daction du devoir**

Une fois tout cela fait, on prend sa copie (totalement vierge √† ce moment) et on commence √† √©crire dessus¬†: on recopie l'introduction, on r√©dige le d√©veloppement directement sur la copie (on ne r√©dige jamais son d√©veloppement sur le brouillon, cela prend beaucoup trop de temps √† recopier). *Le d√©veloppement du devoir doit contenir des titres apparents pour les parties et les sous-parties*. Cela signifie que le titre de votre grande partie est marqu√© dans votre copie (pr√©c√©d√© d'un ¬´¬†I)¬†¬ª) et qu'il est isol√© du texte et soulign√©. Bref, on doit pouvoir voir appara√Ætre d'un coup d'≈ìil votre plan en survolant votre copie du regard.

Comme dit juste au-dessus, si on manque de temps, on coupe une partie du d√©veloppement et on recopie la conclusion qui se trouve sur le brouillon. Attention¬†: si vous ne r√©digez pas tout le d√©veloppement, mettez tout de m√™me le plan apparent pour les parties et sous-parties non d√©velopp√©es. C'est pr√©cis√©ment parce qu'on a une id√©e de ce que vous auriez √©crit qu'il est possible (en cas de gros manque de temps) de ne pas r√©diger tout le d√©veloppement. Si vous ne d√©taillez pas votre plan, c'est la trame de votre raisonnement qui manque et c'est beaucoup plus ennuyeux. Si vous ne pouvez pas r√©diger tout le d√©veloppement, je vous conseille de mettre des √©l√©ments que vous auriez utilis√© sous forme de liste de tirets (en plus des titres apparents qui sont obligatoires).
