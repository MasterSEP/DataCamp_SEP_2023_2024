---
title: "SEP GUIDE - M1 SEP"
format: pdf
output:
  pdf_document:
    latex_engine: xelatex
geometry:
  - margin=0.5in
  - margin=0.5in
  - margin=0.5in
  - margin=0.5in
  - paperwidth=10.67in
  - paperheight=7.5in
header-includes:
   - \usepackage{xcolor}
   - \usepackage{graphicx}
   - \definecolor{lightblue}{RGB}{173,216,230} 
   - \pagecolor{lightblue}
   - \usepackage{fancyhdr}
   - \pagestyle{fancy}
   - \fancyhead[L]{\includegraphics[width=3cm]{SEP.png}}
editor:
  markdown:
    wrap: 72
---

\thispagestyle{empty}

```{=tex}
\vfill  
\begin{center}
Version 2023/24
\vspace*{-2cm}
\end{center}
```
```{=tex}
\newpage
\fancyfoot[C]{}
\renewcommand{\contentsname}{Table des Mati√®res}
\vspace*{-1cm}
\tableofcontents
```
```{=tex}
\newpage
\vspace*{-1cm}
```
<!-- Partie avec contenu Modifiable  -->

# Alg√®bre Lin√©aire : niveau √©l√©mentaire

## Matrices

Une matrice est un tableau de nombres dispos√©s en $m$ lignes et $n$ colonnes. Soit $A$ de taille $(m, n)$ : $$
A = (a_{ij})_{i=1,\ldots,m \atop j=1,\ldots,n} =
\begin{bmatrix}
a_{11} & \cdots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \cdots & a_{mn}
\end{bmatrix}
$$

Le terme $a_{ij}$ est situ√© √† la $i$-√®me ligne de la $j$-√®me colonne de la matrice $A$.

\textcolor{green!70!black!70!black}{Exemples :}

$$
A =
\begin{bmatrix}
-2 & 1 \\
8 & -3 \\
1 & 3
\end{bmatrix}
;\quad
B =
\begin{bmatrix}
-2 \\
-3 \\
5
\end{bmatrix}
;\quad
C =
\begin{bmatrix}
3 & -1 & 6
\end{bmatrix}
;\quad
D =
\begin{bmatrix}
3 & -1 \\
4 & -3
\end{bmatrix}
$$

$A$ est une matrice de taille $(3, 2)$, $B$ est une matrice de taille $(3, 1)$, $C$ est une matrice de taille $(1, 3)$, $D$ est une matrice de taille $(2, 2)$.

Une matrice $(m, 1)$ est dite matrice colonne. Une matrice $(1, n)$ est dite une matrice ligne. Une matrice $(n, n)$ est dite une matrice carr√©e d'ordre $n$.

On appelle transpos√©e de $A$ (et on note $A^t$ ou $A'$), la matrice dont les lignes sont les colonnes de $A$, et dont les colonnes sont les lignes de $A$.

\textcolor{green!70!black}{Exemple :} $$
A =
\begin{bmatrix}
-2 & 1 \\
8 & -3 \\
1 & 3
\end{bmatrix}
\Rightarrow
A^t =
\begin{bmatrix}
-2 & 8 & 1 \\
1 & -3 & 3
\end{bmatrix}
$$

\textcolor{green!70!black}{Propri√©t√©s :} $$
(A + B)^t = A^t + B^t
$$ $$
(AB)^t = B^t A
$$

## Matrices usuelles

Une matrice carr√©e d'ordre $n$ est dite diagonale lorsque, pour tout $i \neq j$, on a $a_{ij} = 0$ : $$
A =
\begin{bmatrix}
-2 & 0 & 0 \\
0 & -3 & 0 \\
0 & 0 & 5
\end{bmatrix}
$$

La matrice unit√© $I_n$ est la matrice diagonale telle que $\forall i = 1, \ldots, n$, $a_{ii} = 1$ : $$
I_3 =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
$$

## Op√©rations de base

Soient $A$ et $B$ deux matrices de m√™me taille $(m, n)$ de termes g√©n√©raux respectifs $a_{ij}$ et $b_{ij}$, et $\lambda$ un nombre r√©el.

$A = B$ si et seulement si $a_{ij} = b_{ij}$ pour tous $i,j$.

La somme de $A$ et $B$ est la matrice de terme g√©n√©ral $a_{ij} + b_{ij}$ : $$
A + B =
\begin{bmatrix}
5 + (-4) & 2 + 1 \\
6 + 1 & 1 + (-3)
\end{bmatrix}
=
\begin{bmatrix}
1 & 3 \\
7 & -2
\end{bmatrix}
$$

Le produit de $A$ par $\lambda$ est la matrice de terme g√©n√©ral $\lambda a_{ij}$ : $$
\lambda A =
3 \times
\begin{bmatrix}
5 & 2 \\
6 & 1
\end{bmatrix}
=
\begin{bmatrix}
15 & 6 \\
18 & 3
\end{bmatrix}
$$

\textcolor{green!70!black}{Exemples :} $$
A =
\begin{bmatrix}
5 & 2 \\
6 & 1
\end{bmatrix}
;\quad
B =
\begin{bmatrix}
-4 & 1 \\
1 & -3
\end{bmatrix}
;\quad
\lambda = 3
$$ $$
A + B =
\begin{bmatrix}
1 & 3 \\
7 & -2 \\
\end{bmatrix}
\hspace{2cm}
\lambda A =
\begin{bmatrix}
15 & 6 \\
18 & -2 \\
\end{bmatrix}
$$

## Multiplication matricielle

Le produit de matrices A et B (not√© $AB$) n'est d√©fini que si le nombre de colonnes de $A$ est √©gal au nombre de lignes de $B$. C'est-√†-dire $A$ doit √™tre de taille $(m, p)$ et $B$ de taille $(p, n)$. Alors $AB$ est de taille $(m, n)$. De plus, soient $a_{ij}$ et $b_{ij}$ les termes g√©n√©raux respectifs de $A$ et $B$, alors le terme g√©n√©ral de $C$ = $AB$ est $c_{ij}$ d√©fini par : $$
c_{ij} = \sum_{k=1}^{p} a_{ik} b_{kj}
$$ \textcolor{green!70!black}{Exemple : } $$A =
\begin{bmatrix}
2 & -2 & 1 \\
-3 & 6 & 8 \\
5 & 2 & 1
\end{bmatrix}
\hspace{2cm}
B = \begin{bmatrix}
-1 & 5 \\
1 & 2 \\
3 & 4
\end{bmatrix}$$ Alors, la matrice $C = AB$ est donn√©e par : $$C = \begin{bmatrix}
-1 & 10 \\
33 & 29 \\
0 & 33
\end{bmatrix}$$

En effet, l'√©l√©ment qui se trouve au croisement de la i-√®me ligne et la j-√®me colonne est : $$c_{ij} = \sum_{k=1}^{p} a_{ik} b_{kj}$$

Par exemple, $$c_{2,1} = (-3) \times (-1) + 6 \times 1 + 8 \times 3 = 33$$

\textbf{\textcolor{red}{Le produit matriciel n‚Äôest pas commutatif.}}

\newpage

## Propri√©t√©s

Le produit est distributif par rapport √† l'addition. C'est-√†-dire : $$
A (B + C) = AB + AC \quad \text{et} \quad (B + C)A = BA + CA
$$

Le produit est associatif, c'est-√†-dire : $$
ABC = A(BC) = (AB)C
$$

Si A est une matrice carr√©e d'ordre n, alors $$A I_n = I_n A = A$$

Le produit de deux matrices peut √™tre nul sans que l'une des deux matrices ne soit la matrice nulle, par exemple : $$
\begin{bmatrix}
1 & 2 \\
2 & 4
\end{bmatrix} \times
\begin{bmatrix}
-2 & 10 \\
1 & -5
\end{bmatrix}
$$

## Inverse

Une matrice carr√©e $A$ d'ordre $n$ est dite inversible lorsqu'il existe une matrice $B$ telle que : $$ A B = B A = I_n $$ $B$ est alors not√©e $A^{-1}$, l'inverse de $A$.

\newpage

## Syst√®mes lin√©aires

Gr√¢ce au produit matriciel, on peut repr√©senter un syst√®me lin√©aire par une √©quation matricielle. Soit le syst√®me lin√©aire de $n$ √©quations et $p$ inconnues :

$$
\begin{cases}
a_{11} x_1 + a_{12} x_2 + \ldots + a_{1p} x_p = b_1 \\
a_{21} x_1 + a_{22} x_2 + \ldots + a_{2p} x_p = b_2 \\
a_{n1} x_1 + a_{n2} x_2 + \ldots + a_{np} x_p = b_n
\end{cases}
$$

On peut le repr√©senter par $AX = B$ o√π :

$$A = \begin{bmatrix}
a_{11} & \ldots & a_{1p} \\
\vdots & \ddots & \vdots \\
a_{n1} & \ldots & a_{np}
\end{bmatrix} ; 
\hspace{2cm}
X = \begin{bmatrix}
x_1 \\
\vdots \\
x_p
\end{bmatrix} ; 
\hspace{2cm}
B = \begin{bmatrix}
b_1 \\
\vdots \\
b_n
\end{bmatrix}$$

\newpage

# Alg√®bre lin√©aire : niveau basique

## D√©terminant

Soit $A = (a_{ij})$ une matrice carr√©e d'ordre 2. Le d√©terminant de $A$ est le r√©el not√© $\text{det}(A)$ tel que : $$
\text{det}(A) = \begin{vmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{vmatrix} = a_{11}a_{22} - a_{12}a_{21}
$$

Soient $A = (a_{ij})$ et $B = (b_{ij})$ deux matrices carr√©es d'ordre $n$ et $\lambda \in \mathbb{R}$. On a les propri√©t√©s suivantes :

-   $\text{det}(AB) = \text{det}(A) \cdot \text{det}(B) = \text{det}(BA)$
-   $\text{det}(A^T) = \text{det}(A)$
-   $\text{det}(\lambda A) = \lambda^n \cdot \text{det}(A)$
-   Si $A$ est diagonale, alors $\text{det}(A) = a_{11}a_{22}\ldots a_{nn}$
-   $\text{det}(I_n) = 1$
-   $A$ est inversible si et seulement si $\text{det}(A) \neq 0$
-   Si $\text{det}(A) \neq 0$, alors $\text{det}(A^{-1}) = \frac{1}{\text{det}(A)}$

## Diagonalisation

\textcolor{green!70!black}{Valeur propre / Vecteur propre}

Une valeur propre de $A$ est un scalaire $\lambda$ tel qu'il existe un vecteur colonne non nul $V$ v√©rifiant : $AV = \lambda V$. $V$ est alors appel√© vecteur propre de $A$ associ√© √† $\lambda$.

\textcolor{green!70!black}{Diagonalisation}

Une matrice carr√©e $A$ d'ordre $n$ est diagonalisable lorsqu'il existe une matrice diagonale $D$ et une matrice inversible $P$ telles que : $A = PDP^{-1}$. $D$ est constitu√©e des valeurs propres de $A$. $P$ est obtenue par la concat√©nation des vecteurs propres de $A$. Si les valeurs propres de $A$ sont distinctes, alors $A$ est diagonalisable (r√©ciproque fausse).

## Matrice sym√©trique

Une matrice carr√©e $A$ d'ordre $n$ est sym√©trique lorsque $A^T = A$. Si $A$ est sym√©trique, alors :

-   $A$ a des valeurs propres r√©elles.
-   $A$ est diagonalisable.
-   Il existe une matrice $P$ telle que $P^{-1} = P^T$ et $A = PDP^{-1}$.

## Produit scalaire

Soit $x$, $y$, $z$ trois vecteurs de $\mathbb{R}^n$ et $\lambda$ un scalaire. On d√©finit par produit scalaire (qu'on note $\langle \cdot, \cdot \rangle$) toute application qui v√©rifie les propri√©t√©s suivantes :

-   $\langle x + \lambda y, z \rangle = \langle x, z \rangle + \lambda \langle y, z \rangle$
-   $\langle x, y + \lambda z \rangle = \langle x, z \rangle + \lambda \langle x, z \rangle$
-   $\langle x, y \rangle = \langle y, x \rangle$
-   $\langle x, x \rangle \geq 0$
-   $\langle x, x \rangle = 0 \Rightarrow x = 0$

Le produit scalaire canonique et usuel est d√©fini comme : $$
\langle x, y \rangle = \sum_{i=1}^{n} x_i \cdot y_i
$$

## Norme

On appelle norme associ√©e √† un produit scalaire le r√©el $\|x\| = \sqrt{\langle x, x \rangle}$. Elle v√©rifie les propri√©t√©s suivantes :

-   $|\langle x, y \rangle| \leq \|x\| \cdot \|y\|$ (In√©galit√© de Cauchy-Schwartz)
-   $\|x + y\| \leq \|x\| + \|y\|$ (In√©galit√© triangulaire)
-   $\|x\| \geq 0$ avec √©galit√© si $x = 0$
-   $\|\lambda x\| = |\lambda| \cdot \|x\|$
-   $\|x + y\|^2 = \|x\|^2 + \|y\|^2 + 2\langle x, y \rangle$

Un vecteur est dit unitaire ou norm√© si $\|x\| = 1$

## Orthogonalit√©

Deux vecteurs $x$ et $y$ sont dits orthogonaux si $\langle x, y \rangle = 0$. On note $x \perp y$.

Une famille de vecteurs $\{x_i\}$ est dite orthogonale si tous ses vecteurs sont deux √† deux orthogonaux. Toute famille orthogonale $\{x_i\}_{i=1}^p$ v√©rifie le th√©or√®me de Pythagore : $$
\left\|\sum_{i=1}^n x_i\right\|^2 = \sum_{i=1}^n \|x_i\|^2
$$

Soit $E$ un espace muni d'un produit scalaire et $X$ une partie de $E$. On appelle orthogonal de $X$ et on note $X^\perp$ l'ensemble : $X^\perp = \{y \in E \,|\, \forall x \in X, \langle x, y \rangle = 0\}$.

On dit que $\{e_i\}_{i=1}^p$ est une base orthonorm√©e de $E$ si et seulement si : - Si $a_1e_1 + a_2e_2 + \ldots + a_ne_n = 0$, alors $a_i = 0$ pour tout $i \in \{1, \ldots, n\}$.\
- Pour tout $x \in E$, il existe $a_1, a_2, \ldots, a_n \in \mathbb{R}$ tels que $x = a_1e_1 + a_2e_2 + \ldots + a_ne_n$.\
- $e_i$ est orthogonal √† $e_j$ pour tout $i \neq j$.\
- Pour tout $i \in \{1, \ldots, n\}$, $\|e_i\| = 1$.

## Projection orthogonale

Soit $x$ un vecteur d'un espace muni d'un produit scalaire $E$. Soit $F$ un sous-espace vectoriel de $E$, $x$ s'√©crit de fa√ßon unique sous la forme : $x = f + f^\perp$ o√π $f \in F$ et $f^\perp \in F^\perp$. On dit que $f$ est le projet√© orthogonal de $x$ sur $F$ et on note $f = P_F(x)$.

Pour $x$ et $y$ de $E$, on a $\langle P_F(x),y\rangle = \langle x, P_F(y)\>$.

Si $(e_1, e_2, \ldots, e_n)$ est une base orthonorm√©e, alors : $P_F(x) = \sum_{i=1}^n \langle x, e_i \rangle e_i$

Notons que : $\|x - P_F(x)\| = \inf_{f \in F} \|x - f\|$

## Matrice orthogonale

Soit $M$ une matrice carr√©e d'ordre $n$. On dit que $M$ est une matrice orthogonale si elle v√©rifie : $$M \cdot M^T = M^T \cdot M = I_n$$

Le d√©terminant d'une matrice orthogonale est √©gal √† $\pm 1$. L'ensemble des valeurs propres de $M$ est inclus dans l'ensemble $\{0, 1\}$.

\newpage

# Fondements de probabilit√© : niveau √©l√©mentaire

## Quelques d√©finitions

-   On appelle √©preuve $E$ toute exp√©rience probabiliste.
-   On appelle univers de $E$ l'ensemble, g√©n√©ralement not√© $\Omega$, de tous les r√©sultats possibles de l'√©preuve $E$ (appel√©s "√©v√©nements √©l√©mentaires").

Lancer une paire de d√©s √©quilibr√©s et en retenir la somme est une √©preuve. $\Omega = \{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\}$ \hspace{1cm} ![](images/deux_des.png){width="5%" fig-align="center"}

## Ev√©nements

Un √©v√©nement est un sous-ensemble de $\Omega$.

-   L'intersection de $A$ et $B$, not√©e $A \cap B$, est un √©v√©nement. Il est r√©alis√© uniquement si $A$ et $B$ se produisent.
-   La r√©union de $A$ et $B$, not√©e $A \cup B$, est un √©v√©nement. Il est r√©alis√© si $A$ ou $B$ se produit. Deux √©v√©nements remarquables sont √† retenir :
    -   L'√©v√©nement certain $\Omega$ ;
    -   L'√©v√©nement impossible $\emptyset$ ;
-   Tous les √©l√©ments qui n'appartiennent pas √† $A$ appartiennent √† un √©v√©nement que l'on appelle le compl√©mentaire de $A$. On le note $\overline{A}$ ou $A^C$.
-   On dit que deux √©v√©nements $A$ et $B$ sont incompatibles s'ils ne peuvent pas √™tre r√©alis√©s en m√™me temps.

Si $A$, $B$ et $C$ sont des √©v√©nements de $\Omega$, les propri√©t√©s suivantes sont toujours v√©rifi√©es :

-   $A \cup \overline{A} = \Omega$ et $A \cap \overline{A} = \emptyset$
-   $\overline{\overline{A} \cap \overline{B}} = \overline{A} \cup \overline{B}$ et $\overline{\overline{A} \cup \overline{B}} = \overline{A} \cap \overline{B}$ (lois de De Morgan)
-   $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
-   $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$

## Partitions

La famille d'√©v√©nements forme une partition de $\Omega$ si : $$
\bigcup_{i \in I} A_i = \Omega \quad \text{et} \quad A_i \cap A_j = \emptyset \text{ pour tout } i \neq j.
$$ Une partition remarquable est la famille qui contient l'√©v√©nement $A$ et son compl√©mentaire.

## Tribus et bor√©liens

![](images/venn_diagrams.jpg){width="50%" fig-align="center"} ![](images/AnBnC.png){width="50%" fig-align="center"}

\textcolor{green!70!black}{Comment pouvons-nous qualifier l'ensemble des √©v√©nements ?}

Une tribu est une famille $T$ de parties de l'ensemble $\Omega$ qui v√©rifie les propri√©t√©s suivantes : - $\Omega \in T$\
- Si $(A_n)$ est une suite d√©nombrable d'√©l√©ments de $T$, alors $\bigcup A_n \in T$\
--- Si $A$ est un √©l√©ment de $T$, alors son compl√©mentaire l'est aussi.

De plus, si $T$ est une tribu, alors : - $\emptyset \in T$\
- Si $(A_n)$ est une suite d'√©l√©ments de $T$, alors $\bigcap A_n \in T$.

\textcolor{green!70!black}{Exemples de tribus :}

Pour le cas discret, on consid√®re l'exp√©rience "Lancer une pi√®ce de monnaie √©quilibr√©e".\
On notera : $P$ pour "PILE appara√Æt" et $F$ pour "FACE appara√Æt".\
Dans ce cas, l'univers est l'ensemble $\{P, F\}$ et $T = \{\Omega, \emptyset, P, F\}$ est une tribu.\
En g√©n√©ral, l'ensemble des parties est une tribu classique.

Pour le cas continu, les intervalles du type $[a, +\infty[$, $]a, +\infty[$, $]-\infty, a[$, $]-\infty, a]$ sont des tribus. Nous les appelons DES BOR√âLIENS.

\textcolor{green!70!black}{Propri√©t√©s :}

Soient $A$ et $B$ deux √©v√©nements. Les propri√©t√©s suivantes sont toujours vraies :

1.  $P(A^C) = 1 - P(A)$\
2.  $P(B) = P(A \cap B) + P(A^C \cap B)$\
3.  Si $A \subset B$, alors $P(A) \leq P(B)$\
4.  $0 \leq P(A) \leq 1$\
5.  $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

De plus, en consid√©rant une suite $(A_k)$ d'√©v√©nements, on a les relations suivantes : $$
\begin{aligned}
P\left(\bigcup A_k\right) & = \lim_{n \to +\infty} P\left(\bigcup_{k=1}^n A_k\right) \\
P\left(\bigcap A_k\right) & = \lim_{n \to +\infty} P\left(\bigcap_{k=1}^n A_k\right) \\
P\left(\bigcup A_k\right) & \leq \sum_{k=1}^{+\infty} P(A_k)
\end{aligned}
$$

Et si $\bigcup A_k = \Omega$, alors : $$
P(B) = \sum_{k=1}^{+\infty} P(B \cap A_k)
$$

## Mesure

Soit $E$ un ensemble muni d'une tribu $T$. On appelle mesure toute application $m : T \rightarrow \mathbb{R}^+$ telle que : - $m(\emptyset) = 0$ - Si $(A_n)$ est une suite d'√©l√©ments de $T$ deux √† deux disjoints alors : $$m\left(\bigcup_n A_n\right) = \sum_n m(A_n).$$

## Probabilit√©s

Soit $E$ un ensemble muni d'une tribu $T$. On appelle probabilit√© toute application $P : T \rightarrow \mathbb{R}^+$ telle que : - $P(\emptyset) = 0$ - Si $(A_n)$ est une suite d'√©l√©ments de $T$ deux √† deux disjoints alors : $$P\left(\bigcup_n A_n\right) = \sum_n P(A_n).$$

## Probabilit√©s conditionnelles

En th√©orie des probabilit√©s, nous nous int√©ressons souvent au comportement d'un al√©a, sachant qu'un autre √©v√©nement est d√©j√† pass√©. C'est ce que nous appelons Les Probabilit√©s Conditionnelles.

Consid√©rant deux √©v√©nements de probabilit√© non nulle, $A$ et $B$, la probabilit√© conditionnelle de $A$ sachant que $B$ est r√©alis√© (couramment dit $A$ sachant $B$) est donn√©e par : $$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

Par commutativit√© de l'intersection, nous avons : $$
P(A \cap B) = P(B \cap A)
$$

En utilisant la formule ci-dessus, nous pouvons √©galement exprimer la probabilit√© conditionnelle de $B$ sachant $A$ : $$
P(B|A) = \frac{P(A|B) \cdot P(B)}{P(A)}
$$

C'est ce que nous appelons \textcolor{red}{LA FORMULE DE BAYES}.

## Ind√©pendance

Deux √©v√©nements $A$ et $B$ sont dits ind√©pendants si et seulement si : $$P(A \cap B) = P(A) \cdot P(B)$$ En termes courants, deux √©v√©nements sont ind√©pendants si le r√©sultat de l'un n'influence aucunement l'aboutissement de l'autre. Sous condition d'ind√©pendance de $A$ et $B$, la notion de la probabilit√© conditionnelle tombe √† l'eau, car les √©v√©nements √©voluent l'un sans se soucier de l'autre. Ceci se traduit par : $$P(A|B) = P(A)$$ $$P(B|A) = P(B)$$

Notons que si $A$ est ind√©pendant de $B$, il le sera par rapport √† son compl√©mentaire √©galement, et vice versa. En g√©n√©ral, pour une suite $(A_n)$ d'√©v√©nements ind√©pendants, on a : $$P\left(\bigcap A_i\right) = \prod P(A_i) = P(A_1) \cdot \ldots \cdot P(A_n)$$ Cette formule est largement utilis√©e en statistique.

\textcolor{red}{NE PAS CONFONDRE IND√âPENDANCE ET INCOMPATIBILIT√â DES √âV√âNEMENTS}

## Variable al√©atoire

Une variable al√©atoire est un nombre qui d√©pend du r√©sultat d'une exp√©rience al√©atoire. Chaque ex√©cution de l'exp√©rience g√©n√®re une r√©alisation de la variable al√©atoire.

Math√©matiquement, on d√©finit une variable al√©atoire X comme une fonction $X : T \rightarrow \mathbb{R}$ qui associe √† chaque √©v√©nement s, un r√©el $X(s)$.

Par exemple, dans une queue pour la caisse d'un magasin, le nombre de clients est une variable al√©atoire. La dur√©e de traitement de chaque requ√™te aussi. Remarquons que la premi√®re est un nombre entier. On dit qu'elle est √† support discret. Alors que la deuxi√®me est une dur√©e (un nombre r√©el). On dit qu'elle est √† support continu.

\textcolor{green!70!black}{Qu‚Äôest- ce qui caract√©rise une variable al√©atoire ?}

## Fonction de r√©partition

Une variable al√©atoire traduit le r√©sultat d'une exp√©rience al√©atoire en nombre r√©el. La fonction de r√©partition transporte le calcul des probabilit√©s concernant les r√©alisations de la variable al√©atoire. C'est la fonction d√©finie par : $$F_X(x) = P(X \leq x)$$ \textcolor{green!70!black}{Propri√©t√©s :}

Pour tout $x$, $0 \leq F_X(x) \leq 1$ $F_X$ est une fonction croissante. $\lim_{x \to -\infty} F_X(x) = 0$ et $\lim_{x \to \infty} F_X(x) = 1$

![](images/fonction_repartition.png){width="50%" fig-align="center"}

## Probabilit√© ponctuelle / Densit√©

\textcolor{green!70!black}{CAS DISCRET : Probabilit√© ponctuelle}

La probabilit√© ponctuelle est la fonction qui d√©crit les sauts de la fonction de r√©partition : $$P(X = k) = P(X \leq k) - P(X \leq k - 1) = p_k$$ $$\sum p_i = 1$$

\textcolor{green!70!black}{CAS CONTINU : densit√© de probabilit√©}

La densit√© est la fonction qui d√©crit les variations de la fonction de r√©partition : $$f(x) = \frac{dF_X}{dx}(x)$$ $$\int f(x) dx = 1$$

\newpage

# Fondements de probabilit√©s : niveau basique

## Moments

\textcolor{green!70!black}{ESP√âRANCE}

L'esp√©rance d'une variable al√©atoire est sa valeur attendue. C'est une mesure de localisation de la distribution.

Dans le cas discret : $$E(X) = \sum k \cdot P(X = k)$$ $$k \in X(\Omega)$$ Alors que dans le cas continu : $$E(X) = \int x \cdot f_X(x) \, dx$$ $$x \in X(\Omega)$$

\textcolor{green!70!black}{TH√âOR√àME DE TRANSFERT} $$E(g(X)) = \sum g(k) \cdot P(X = k)$$ $$\forall k \in X(\Omega)$$ $$E(g(X)) = \int g(x) \cdot f_X(x) \, dx$$ $$\forall x \in X(\Omega)$$ \textcolor{green!70!black}{VARIANCE}

La variance d'une variable al√©atoire d√©crit la dispersion de la variable al√©atoire autour de sa valeur moyenne (son esp√©rance).

Elle est d√©finie par : $$Var(X) = E(X^2) - (E(X))^2 = E((x - E(X))^2)$$ Sa racine carr√©e est appel√©e √©cart-type et not√©e g√©n√©ralement : $$\sigma(X) = \sqrt{Var(X)}$$

\newpage

\textcolor{green!70!black}{CENTRAGE ET R√âDUCTION}

Le centrage consiste √† localiser la distribution autour de l'origine et la r√©duction consiste √† normaliser la dispersion. La technique est simple : $$Y = \frac{X - E(X)}{\sigma(X)}$$

\textcolor{green!70!black}{MOMENTS D‚ÄôORDRE r}

Le moment d'ordre r est d√©fini par : $$\mu_r = E(X^r)$$ Le moment centr√© d'ordre r est d√©fini ainsi : $$\muÃÉ_r = E((X - E(X))^r)$$

## Couples al√©atoires

La fonction conjointe $$F_{X, Y}(x, y) = P(X \leq x \cap Y \leq y)$$ est appel√©e la distribution conjointe de X et Y.

Dans le cas continu, la fonction d√©finie par : $$f_{X, Y}(x, y) = \frac{\partial^2 F_{X, Y}(x, y)}{\partial x \partial y}$$ est la densit√© conjointe du couple (X, Y). On a donc : $$F_{X, Y}(x, y) = \int \int f_{X, Y}(t, u) \, dt \, du, \text{ o√π } -\infty < x, y < +\infty,$$

Dans le cas discret, on d√©finit la fonction de probabilit√© conjointe : $$P(X = x_i, Y = y_j) = p_{ij}$$ On a donc : $$F_{X, Y}(x, y) = \sum \sum p_{ij}, \text{ o√π } x_i \leq x \text{ et } y_j \leq y$$

\textcolor{green!70!black}{LOI MARGINALE}

La loi marginale de X est d√©finie comme suit : $$f_X(x) = \int f_{X, Y}(x, y) \, dy, \text{ o√π } -\infty < x < \infty,$$ dans le cas continu, ou encore : $$f_X(x_i) = \sum p_{ij}, \text{ o√π } j \text{ tel que } y_j \leq y$$

Si X et Y sont ind√©pendants, alors : $$f_{X, Y}(x, y) = f_X(x) \cdot f_Y(y)$$

\textcolor{green!70!black}{COVARIANCE}

La covariance mesure l'intensit√© de la relation lin√©aire entre deux variables al√©atoires X et Y. Elle est d√©finie comme suit : $$Cov(X, Y) = E(XY) - E(X) \cdot E(Y)$$

Si X et Y sont ind√©pendants, alors : $$Cov(X, Y) = 0$$

\textcolor{red}{Il est important de noter que la r√©ciproque n'est pas vraie : la covariance n'implique pas n√©cessairement l'ind√©pendance entre X et Y.}

\newpage

## Propri√©t√©s

\textcolor{green!70!black}{ESP√âRANCE} $$
\mathbb{E}(aX + bY) = a\mathbb{E}(X) + b\mathbb{E}(Y)
$$ $$
\mathbb{E}(a) = a
$$

\textcolor{green!70!black}{VARIANCE} $$
\text{Var}(aX) = a^2\text{Var}(X)
$$ $$
\text{Var}(a) = 0
$$ $$
\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)
$$ $$
\text{Var}(X - Y) = \text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X,Y)
$$

\textcolor{green!70!black}{COVARIANCE} $$
\text{Cov}(X, Y) = \text{Cov}(Y, X)
$$ $$
\text{Cov}(aX + b, cY + d) = ac\text{Cov}(X, Y)
$$ $$
\text{Cov}(aX + bY, U) = a\text{Cov}(X, U) + b\text{Cov}(Y, U)
$$ $$
\text{Cov}(X, cU + dV) = c\text{Cov}(X, U) + d\text{Cov}(X, V)
$$ $$
\text{Cov}(aX + bY, cU + dV) = ac\text{Cov}(X, U) + ad\text{Cov}(X, V) + bc\text{Cov}(Y, U) + bd\text{Cov}(Y, V)
$$

\newpage

## Vecteurs al√©atoires

Pour un vecteur al√©atoire $$(X_1, X_2, \ldots, X_n)$$, l'esp√©rance est toujours lin√©aire. Pour une suite $$(a_i)_{i \in \{1, \ldots, n\}}$$ de r√©els, on a : $$
\mathbb{E}(a_1X_1 + a_2X_2 + \ldots + a_nX_n) = a_1\mathbb{E}(X_1) + a_2\mathbb{E}(X_2) + \ldots + a_n\mathbb{E}(X_n)
$$

Si les variables al√©atoires $$X_1, X_2, \ldots, X_n$$ sont ind√©pendantes, alors la variance de leur somme est √©gale √† la somme de leurs variances individuelles : $$
\text{Var}(X_1 + X_2 + \ldots + X_n) = \text{Var}(X_1) + \ldots + \text{Var}(X_n)
$$

## Lois usuelles

Ces tableaux r√©capitulent les lois usuelles que vous pourrez rencontrer dans diff√©rents cours du master.

```{=tex}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Nom & Notation & $X(\Omega)$ & $P(X = k)$ & $E[X]$ & $V(X)$ \\
\hline
Uniforme & $X \sim U(\{1, 2, \ldots, n\})$ & $\{1, 2, \ldots, n\}$ & $\frac{1}{n}$ & $\frac{n+1}{2}$ & $\frac{n^2-1}{12}$ \\
\hline
Bernouilli & $X \sim B(p), 0 < p < 1$ & $\{0, 1\}$ & $P(X = 1) = p$ & $p$ & $p(1-p)$ \\
& & & $P(X = 0) = 1 - p$ & & \\
\hline
Binomiale & $X \sim B(n, p), 0 < p < 1$ & $\{1, 2, \ldots, n\}$ & $C_k^n p^k (1 - p)^{n-k}$ & $np$ & $np(1-p)$ \\
\hline
G√©om√©trique & $X \sim G(p), 0 < p < 1$ & $\mathbb{N}$ & $p(1-p)^{k}$ & $\frac{1-p}{p}$ & $\frac{1-p}{p^2}$ \\
\hline
Poisson & $X \sim P(\lambda), \lambda > 0$ & $\mathbb{N}$ & $\frac{\lambda^k}{k!}e^{-\lambda}$ & $\lambda$ & $\lambda$ \\
\hline
\end{tabular}
```
```{=tex}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Nom & Notation & $X(\Omega)$ & $f_X(x)$ & $E[X]$ & $V(X)$ \\
\hline
Uniforme & $X \sim U([a, b]), a < b$ & $[a, b]$ & $\frac{1}{b - a}1_{[a, b]}(x)$ & $\frac{a + b}{2}$ & $\frac{(a - b)^2}{12}$ \\
\hline
Exponentielle & $X \sim \mathcal{E}(\lambda), \lambda > 0$ & $\mathbb{R}^+$ & $\lambda e^{-\lambda x}1_{\mathbb{R}^+}(x)$ & $\frac{1}{\lambda}$ & $\frac{1}{\lambda^2}$ \\
& $\mathcal{E}(Œª) = Œ≥(1, Œª)$ & & & & \\
\hline
Normale ou Gaussienne & $X \sim N(m, \sigma^2), \sigma > 0$ & $\mathbb{R}$ & $\frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x - m)^2}{2\sigma^2}\right)$ & $m$ & $\sigma^2$ \\
\hline
Gamma & $X \sim \gamma(\alpha, \theta), \alpha > 0, \theta > 0$ & $\mathbb{R}^+$ & $\frac{\theta^\alpha}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\theta x}1_{\mathbb{R}^+}(x)$ & $\frac{\alpha}{\theta}$ & $\frac{\alpha}{\theta^2}$ \\
& $\Gamma(\alpha) = \int_0^\infty e^{-x}x^{\alpha-1} \, dx$ & & & & \\
\hline
Khi-2 & $X \sim \chi^2(n), n \in \mathbb{N}^+$ & $\mathbb{R}^+$ & $\gamma\left(\frac{n}{2}, \frac{1}{2}\right)$ & $n$ & $2n$ \\
& $Y_1, Y_2, \ldots, Y_n \text{ ind√©pendantes},$ & & & & \\
& $Y_i \sim \mathcal{N}(0, 1), \quad X = \sum_{i=1}^{n} Y_i^2$ & & & & \\
\hline
B√™ta & $X \sim B(\alpha, \theta), \alpha > 0, \theta > 0$ & $[0, 1]$ & $\frac{x^{\alpha-1}(1-x)^{\theta-1}}{B(\alpha, \theta)}1_{[0, 1]}(x)$ & $\frac{\alpha}{\alpha+\theta}$ & $\frac{\alpha\theta}{(\alpha+\theta)^2(\alpha+\theta+1)}$ \\
& $B(\alpha, \theta) = \int_0^1 x^{\alpha-1}(1-x)^{\theta-1} \, dx$ & & & & \\
& $B(\alpha, \theta) = \int_0^1 x^{\alpha-1}(1-x)^{\theta-1} \, dx$ & & & & \\
& $X = \frac{Z}{1 + Z}, \quad Z \sim B'(\alpha, \theta)$ & & & & \\
\hline
B√™ta (prime) & $Z ‚àº B'(Œ±, Œ∏), Œ± > 0, Œ∏ > 0$ & $\mathbb{R}^+$ & $\frac{z^{Œ±-1}}{B(Œ±,Œ∏) \cdot (1+z)^{Œ±+Œ∏}} \cdot 1_{\mathbb{R}^+}(z)$ & $ \frac{Œ±}{Œ∏ - 1}$ & $\frac{Œ±(Œ±+Œ∏-1)}{(Œ∏-1)^2(Œ∏-2)}$ \\
& $X \sim \gamma(\alpha, 1), Y \sim \gamma(\theta, 1), X \perp\!\!\!\perp Y$ & & & & \\
& $Z = \frac{X}{Y}$ & & & $Œ∏ > 1$ & $Œ∏ > 2$ \\
& $B(Œ±, Œ∏) = \int_{0}^{\infty} \frac{x^{\alpha-1}}{(1+x)^{\alpha+\theta}} \, dx$ & & & & \\
\hline
Student T & $T \sim T(n), n \in \mathbb{N}^*$ & $\mathbb{R}$ & $\frac{\left(1 + \frac{t^2}{n}\right)^{-(n+\frac{1}{2})}}{\sqrt{n}\cdot B(\frac{1}2,\frac{n}2)}$ & $0$ & $\frac{n}{n - 2}$ \\
& $X \sim \mathcal{N}(0, 1), Y \sim \chi^2(n), X \perp\!\!\!\perp Y$ & & & & \\
& $T = \frac{X}{\sqrt{Y/n}} $ & & & & $n > 2$ \\
& $T^2/n \sim B'(1/2, n/2)$ & & & & \\
\hline
Fisher & $X \sim F(n, m)$ & $\mathbb{R}^+$ & $\frac{Kx^{n/2-1}}{(m+nx)^{(n+m)/2}}1_{\mathbb{R}^+}(x)$ & $\frac{m}{m-2}$ & $\frac{2m^2(n+m-2)}{n(m-2)^2(m-4)}$ \\
& $N \sim \chi^2(n), n \in \mathbb{N}^*$ & & $K = \frac{n^{\frac{n}{2}} \cdot m^{\frac{m}{2}}}{B\left(\frac{n}{2}, \frac{m}{2}\right)}$ & & \\
& $M \sim \chi^2(m), \quad m \in \mathbb{N}^*$ & & & $m > 2$ & $m > 4$ \\
& $N \perp\!\!\!\perp M, \quad X = \frac{N}{n} / \frac{M}{m}$ & & & & \\
& $\frac{n}{m}X \sim B' \left(\frac{n}{2}, \frac{m}{2}\right)$ & & & & \\
\hline
\end{tabular}
```
\newpage

$$\Gamma(x) = \int_0^{\infty} t^{x-1}e^{-t} \, dt \text{ : d√©signe la fonction Gamma d'Euler}$$

$$B(x, y) = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)} \text{ : d√©signe la fonction B√™ta}$$

Nous allons souvent rencontrer les lois gris√©es dans les Tests statistiques. L√† encore, conna√Ætre les densit√©s ne servirait pas √† grand chose, mais ceci nous √©vitera de parler de lois dont nous ne connaissons pas la t√™te.

\newpage

# Fondements de probabilit√©s

## Vecteurs al√©atoires

\textcolor{green!70!black}{INDEPENDANCE DEUX √Ä DEUX}

Les variables $X_1, \ldots, X_n$ sont deux √† deux ind√©pendantes si et seulement si : $\forall i \neq j, X_i$ et $X_j$ sont ind√©pendantes.

\textcolor{green!70!black}{INDEPENDANCE MUTUELLE}

Les variables $X_1, \ldots, X_n$ sont mutuellement ind√©pendantes si et seulement si : $$P(X_1 = x_1, \ldots, X_n = x_n) = P(X_1 = x_1) \times \ldots \times P(X_n = x_n)$$

\textcolor{red}{L'IND√âPENDANCE MUTUELLE IMPLIQUE L'IND√âPENDANCE DEUX √Ä DEUX. LA R√âCIROQUE EST FAUSSE.}

Si $X_1, \ldots, X_n$ sont mutuellement ind√©pendantes, alors pour toute famille de fonctions r√©elles $f_i$, on a : $(f_1(X_1), \ldots, f_n(X_n))$ sont ind√©pendantes.

\textbf{ESP√âRANCE ET VARIANCE-COVARIANCE}

Soit $X = (X_1, \ldots, X_n)^T$ un vecteur al√©atoire. Dans le cas multidimensionnel, l'esp√©rance scalaire est remplac√©e par un vecteur esp√©rance. $$E(X) = (E(X_1), \ldots, E(X_n))^T$$

La variance unidimensionnelle est remplac√©e par la matrice sym√©trique de variance-covariance. Elle contient les variances en diagonale et les covariances ailleurs. On la note g√©n√©ralement $\Sigma_X$. $$\Sigma_X = \begin{bmatrix}
V(X_1) & \ldots & \text{Cov}(X_1, X_n) \\
\vdots & \ddots & \vdots \\
\text{Cov}(X_n, X_1) & \ldots & V(X_n)
\end{bmatrix}$$

## Notions de convergence

Si l'on pense √† des donn√©es, vues comme r√©alisation de variables al√©atoires $X_1, \ldots, X_n$, il serait int√©ressant de se poser la question de savoir comment √©volue cette suite lorsque $n$ tend vers l'infini.

\textcolor{green!70!black}{Convergence presque s√ªre}

On dit que $(X_n)$ converge presque s√ªrement vers $X$ et on note $X_n \xrightarrow{\text{p.s.}} X$ si et seulement si : $P\left(\lim_{{n\to+\infty}} X_n = X\right) = 1$

\textcolor{green!70!black}{Convergence en probabilit√©}

On dit que $(X_n)$ converge en probabilit√© vers $X$ et on note $X_n \xrightarrow{\text{p}} X$ si et seulement si : $\forall \varepsilon > 0, \quad P(|X_n - X| > \varepsilon) \rightarrow 0$

\textcolor{green!70!black}{Convergence en Loi}

On dit que $(X_n)$ converge en loi vers $X$ et on note $X_n \xrightarrow{\mathcal{L}} X$ si et seulement si : $F_{X_n} \xrightarrow{n \to +\infty} F_X$ O√π $F_X$ d√©note la fonction de r√©partition de $X$.

\textcolor{green!70!black}{Convergence en Moyenne quadratique}

On dit que $(X_n)$ converge en moyenne quadratique vers $X$ et on note $X_n \xrightarrow{m.q.} X$ si et seulement si : $\mathbb{E}((X_n - X)^2) \rightarrow 0$ Cette d√©finition peut se g√©n√©raliser jusqu'√† l'ordre $n$, mais nous n'en aurons pas besoin.

![](images/convergence.png){width="30%" fig-align="center"}

## Loi faible des grands nombres

Soit $X_1, \ldots, X_n$ une suite de variables al√©atoires ind√©pendantes et de m√™me loi telles que : $\mathbb{E}(X_i) = \mu$ et $\text{Var}(X_i) = \sigma^2$ alors :\
$$\frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{p} \mu $$

## Loi forte des grands nombres

Soit $X_1, \ldots, X_n$ une suite de variables al√©atoires ind√©pendantes et de m√™me loi telles que : $\mathbb{E}(X_i) = \mu$ et $\text{Var}(X_i) = \sigma^2$, alors :\
$$\frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{p.s.} \mu$$

## Th√©or√®me Central Limite

Soit $X_1, \ldots, X_n$ une suite de variables al√©atoires ind√©pendantes et de m√™me loi telles que : $\mathbb{E}(X_i) = \mu$ et $\text{Var}(X_i) = \sigma^2$, alors : $$\sqrt{n}\frac{\overline{X}_n - \mu}{\sigma} \xrightarrow{\mathcal{Loi}} \mathcal{N}(0, 1)$$

\newpage

# Statistique inf√©rentielle : niveau basique

## Echantillon / Estimateur

Le point de d√©part est un vecteur (ou un tableau dans le cas multidimensionnel) de donn√©es. Ces donn√©es peuvent √™tre vues comme les r√©alisations $(x_1, x_2, \ldots, x_n)$ d'une variable al√©atoire $X$ qui d√©pend d'un certain param√®tre $\theta$ que nous allons chercher √† estimer. Pour ce faire, nous allons construire un √©chantillon de cette variable. Un √©chantillon $(X_1, X_2, \ldots, X_n)$ est un n-uplet de variables al√©atoires ind√©pendantes qui suivent toutes la m√™me loi (celle de $X$). Un estimateur de $\theta$ est une fonction $\hat{\theta} = f(X_1, X_2, \ldots, X_n)$ de notre √©chantillon, qui poss√®de une loi de probabilit√©. Lorsque l'al√©a est r√©alis√©, $\hat{\theta}(\omega) = f(x_1, x_2, \ldots, x_n)$ est une estimation de $\theta$. Le but de ce cours est de construire le meilleur estimateur possible de $\theta$.

## Estimateur sans biais

Pour que l'estimation soit bonne, il faut que $\hat{\theta}$ soit proche de $\theta$. Comme $\hat{\theta} = f(X_1, X_2, \ldots, X_n)$ est une variable al√©atoire, on ne peut imposer de condition qu'√† sa valeur moyenne.

On d√©finit ainsi le biais : $$b_n(\hat{\theta}, \theta) = \mathbb{E}(\hat{\theta}_n) - \theta$$

Un estimateur est dit sans biais si $b_n(\hat{\theta}, \theta) = 0$, c'est-√†-dire : $$\mathbb{E}(\hat{\theta}_n) = \theta$$

## Estimateur convergent

Un estimateur est dit convergent s'il converge en probabilit√© vers le param√®tre √† estimer : $$\hat{\theta}_n \xrightarrow{P} \theta$$

En pratique, tout estimateur sans biais et dont la variance tend vers 0 est convergent.

## Estimateur optimal

\textcolor{green!70!black}{Qualit√© d‚Äôun estimateur}

La qualit√© d'un estimateur est mesur√©e √† travers son erreur quadratique moyenne d√©finie par : $$EQM(\hat{\theta}_n) = (b_n(\hat{\theta}, \theta))^2 + V(\hat{\theta}_n)$$ Comme nous cherchons tout le temps (presque) des estimateurs sans biais, il reste √† comparer les variances.

Un estimateur ùúÉÃÇ1 est meilleur que ùúÉÃÇ2 si : $$V(\hat{\theta}_1) < V(\hat{\theta}_2)$$

\textcolor{green!70!black}{In√©galit√© de Rao-Cramer/ Efficacit√©}

On d√©finit la quantit√© d'information apport√©e par l'estimateur par : $$
I(\hat{\theta}_n) = -\left( \mathbb{E} \left( \frac{\partial L}{\partial \theta} \right) \right)^2
$$ O√π ùêø(ùë•, ùúÉ) = ‚àè ùëì(ùë•ùëñ) (nous reviendrons sur sa d√©finition)

L'in√©galit√© de Rao-Cramer postule que la variance d'un estimateur ne peut pas aller en del√† d'un certain seuil : $$V(\hat{\theta}_n) \geq \frac{1}{I(\hat{\theta}_n)}$$ Un estimateur est optimal (ou efficace) si sa variance v√©rifie le cas d'√©galit√©.

\newpage

## Construction d'un estimateur

\textcolor{green!70!black}{M√©thode du maximum de vraisemblance}

La m√©thode du maximum de vraisemblance consiste √† affecter $ùúÉ$ la valeur qui maximise la probabilit√© d'observer $(ùë•_1, ùë•_2, ‚Ä¶ , ùë•_ùëõ)$ lorsque l'al√©a du vecteur $(ùëã_1, ùëã_2, ‚Ä¶ , ùëã_ùëõ)$ tombe. Sans trop rentrer dans la th√©orie de la vraisemblance, nous allons pr√©senter un algorithme en cinq √©tapes pour calculer cet estimateur (qui pr√©sente des propri√©t√©s assez s√©duisantes) :

\textcolor{blue}{Etape 1 : Calculer la fonction de vraisemblance}

Dans le cas continu : $$L(\mathbf{x}, \theta) = \prod_{i=1}^{n} f(x_i)$$

Dans le cas discret : $$L(\mathbf{x}, \theta) = \prod_{i=1}^{n} P(X_i = x_i)$$ \textcolor{blue}{Etape 2 : Calculer le log-vraisemblance}

Il s'agit de calculer un maximum, ce qui revient √† d√©river. Il s'agit ici d'un produit de n facteurs, ce qui rend la d√©rivation assez coriace. La fonction logarithmique pr√©sente des propri√©t√©s assez sympas pour faciliter cette t√¢che.

\textcolor{blue}{Etape 3 : Calculer la d√©riv√©e de la log-vraisemblance}

\textcolor{blue}{Etape 4 : R√©soudre l'√©quation d'inconnue $ùúΩ$} $$\frac{\partial (\ln(L))}{\partial \theta} = 0 \Rightarrow \theta = \theta_0$$

\textcolor{blue}{Etape 5 : V√©rifier qu'il s'agit d'un maximum.}

En s'assurant que : $$\frac{\partial^2 (\ln(L))}{\partial \theta^2} < 0$$

\newpage

\textcolor{green!70!black}{M√©thode des moments}

Comme le param√®tre √† estimer intervient dans la densit√© de probabilit√©, les moments th√©oriques sont souvent en fonction de ce param√®tre. Ainsi, la m√©thode des moments consiste √† √©galiser les moments th√©oriques (esp√©rance, variance) √† leurs √©quivalents empiriques et √† en d√©gager une estimation ponctuelle.

En pratique, il faut r√©soudre l'(les) √©quation(s) : $$\mathbb{E}(X) = \overline{X} \text{ et } \text{Var}(X) = S_n^2$$ avec : $$\overline{X} = \frac{1}{n} \sum_{i=1}^{n} X_i \hspace{2cm}
 S_n^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2$$

\textcolor{green!70!black}{M√©thode des moindres carr√©s ordinaires}

Lorsqu'il s'agit de prendre une mesure ùúÉ avec un appareil dot√© d'une impr√©cision $ùúÄ$, alors le probl√®me d'estimation peut s'√©crire : $ùëã = ùúÉ + ùúÄ$. La m√©thode des moindres-carr√©s ordinaires consiste √† trouver le param√®tre $ùúÉ$ qui minimise la somme des carr√©es des erreurs : $$ùúÉ_{ùëÄùê∂ùëÇ} = \arg\min \left( \sum_{i=0}^n \varepsilon_i^2 \right) = \arg\min \left( \sum_{i=0}^n (X_i - \theta)^2 \right)$$

## Intervalles de confiance

Un intervalle de confiance \[$A$, $B$\] de niveau $1 - \alpha$ est un intervalle al√©atoire qui a la probabilit√© $1 - \alpha$ de contenir le param√®tre √† estimer $\theta$. Formellement, on √©crit : $P (t_1 (\theta) \leq f(X_1, \ldots, X_n) \leq t_2 (\theta)) = P(A \leq \theta \leq B) = 1 - \alpha$

\newpage

## Test d'hypoth√®ses

Dans le cadre d'un test d'hypoth√®se, nous cherchons √† faire valoir une hypoth√®se en d√©pit d'une autre, qui lui est contradictoire.

On appellera la premi√®re (celle dont le rejet √† tort sera le plus pr√©judiciable) ¬´ Hypoth√®se nulle ¬ª et la deuxi√®me ¬´ Hypoth√®se alternative ¬ª.

![](images/test_hypo.png){width="50%" fig-align="center"}

Les calculs qui se cachent derri√®re le choix de l'hypoth√®se √† garder sont compliqu√©s. Mais BONNE NOUVELLE, la machine fera tour √† notre place. Il suffit juste de suivre correctement la m√©thode :

\textcolor{blue}{Etape 1 : Choisir judicieusement les hypoth√®ses √† √©valuer et fixer le risque $ùõº$}\
\textcolor{blue}{Etape 2 : Choisir le test adapt√© √† la proc√©dure}\
\textcolor{blue}{Etape 3 : Rentrer la commande correspondante sur R et ex√©cuter}\
\textcolor{blue}{Etape 4 : Lire dans les sorties la p-value. si elle est sup√©rieure √† Œ± on accepte H0. Si elle lui est inf√©rieure, on rejette H0}

\newpage

## Construction d'intervalles de confiance

Les intervalles de confiance sont des outils essentiels en statistique pour estimer des param√®tres inconnus tout en mesurant l'incertitude associ√©e √† cette estimation. Ci-dessous, vous trouverez un tableau pr√©sentant la construction des intervalles de confiance pour diff√©rents param√®tres.

```{=tex}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{images/intervalles_conf.png}
\end{figure}
```
